import logging
from typing import Dict, List, Optional, Tuple
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor
import os
import re

import pandas as pd

from ..data_models import Product, MatchResult, ProcessingResult
from ..matching.text_matcher import TextMatcher
from ..matching.image_matcher import ImageMatcher
from ..matching.multimodal_matcher import MultiModalMatcher
from ..scraping.koryo_scraper import KoryoScraper
from ..scraping.naver_crawler import NaverShoppingCrawler
from utils.caching import FileCache

from .excel_manager import ExcelManager
from .data_cleaner import DataCleaner
from .product_factory import ProductFactory
from .file_splitter import FileSplitter

class ProductProcessor:
    """ì œí’ˆ ë°ì´í„° ì²˜ë¦¬ë¥¼ ìœ„í•œ ë©”ì¸ í´ë˜ìŠ¤"""
    
    def __init__(self, config: Dict):
        """
        ì œí’ˆ í”„ë¡œì„¸ì„œ ì´ˆê¸°í™”
        
        Args:
            config: ì• í”Œë¦¬ì¼€ì´ì…˜ ì„¤ì •
        """
        self.config = config
        self.logger = logging.getLogger(__name__)
        self.progress_callback = None  # ì§„í–‰ìƒí™© ì½œë°± ì´ˆê¸°í™”
        self._init_components()
        
        # ë°°ì¹˜ ì²˜ë¦¬ ì„¤ì •
        self.batch_size = config['PROCESSING'].get('BATCH_SIZE', 10)
    
    def _init_components(self):
        """í•„ìš”í•œ ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”"""
        # ìºì‹œ ì´ˆê¸°í™”
        self.cache = FileCache(
            cache_dir=self.config['PATHS']['CACHE_DIR'],
            duration_seconds=self.config['PROCESSING']['CACHE_DURATION'],
            max_size_mb=self.config['PROCESSING'].get('CACHE_MAX_SIZE_MB', 1024),
            enable_compression=self.config['PROCESSING'].get('ENABLE_COMPRESSION', False),
            compression_level=self.config['PROCESSING'].get('COMPRESSION_LEVEL', 6)
        )
        
        # ë§¤ì¹­ ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.text_matcher = TextMatcher(cache=self.cache)
        
        self.image_matcher = ImageMatcher(
            cache=self.cache,
            similarity_threshold=self.config['MATCHING']['IMAGE_SIMILARITY_THRESHOLD']
        )
        
        self.multimodal_matcher = MultiModalMatcher(
            text_weight=self.config['MATCHING']['TEXT_WEIGHT'],
            image_weight=self.config['MATCHING']['IMAGE_WEIGHT'],
            text_matcher=self.text_matcher,
            image_matcher=self.image_matcher,
            similarity_threshold=self.config['MATCHING'].get('TEXT_SIMILARITY_THRESHOLD', 0.75)
        )
        
        # ìŠ¤í¬ë˜í¼ ì´ˆê¸°í™”
        self.koryo_scraper = KoryoScraper(
            max_retries=self.config['PROCESSING']['MAX_RETRIES'],
            cache=self.cache,
            timeout=self.config['PROCESSING'].get('REQUEST_TIMEOUT', 30)
        )
        
        # í”„ë¡ì‹œ ì‚¬ìš© ì—¬ë¶€ í™•ì¸
        use_proxies = False
        if 'NETWORK' in self.config and self.config['NETWORK'].get('USE_PROXIES') == 'True':
            use_proxies = True
            self.logger.info("í”„ë¡ì‹œ ì‚¬ìš© ëª¨ë“œë¡œ ë„¤ì´ë²„ í¬ë¡¤ëŸ¬ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.")
            
        self.naver_crawler = NaverShoppingCrawler(
            max_retries=self.config['PROCESSING']['MAX_RETRIES'],
            cache=self.cache,
            timeout=self.config['PROCESSING'].get('REQUEST_TIMEOUT', 30),
            use_proxies=use_proxies
        )
        
        # ìŠ¤í¬ë˜í¼ ì„¤ì • ì ìš©
        scraping_config = self.config.get('SCRAPING', {})
        if scraping_config:
            self._configure_scrapers(scraping_config)
        
        # ìŠ¤ë ˆë“œí’€ ì´ˆê¸°í™”
        self.executor = ThreadPoolExecutor(
            max_workers=self.config['PROCESSING']['MAX_WORKERS'],
            thread_name_prefix='ProductProcessor'
        )
        
        # ìœ í‹¸ë¦¬í‹° ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.excel_manager = ExcelManager(self.config, self.logger)
        self.data_cleaner = DataCleaner(self.config, self.logger)
        self.product_factory = ProductFactory(self.config, self.logger, self.data_cleaner)
        self.file_splitter = FileSplitter(self.config, self.logger)
    
    def _configure_scrapers(self, scraping_config: Dict):
        """ìŠ¤í¬ë˜í¼ ì„¤ì • ì ìš©"""
        scrapers = [self.koryo_scraper, self.naver_crawler]
        
        for scraper in scrapers:
            # Max workers ì„¤ì •
            max_workers = scraping_config.get('MAX_CONCURRENT_REQUESTS', 5)
            if hasattr(scraper, 'executor') and hasattr(scraper.executor, '_max_workers'):
                scraper.executor._max_workers = max_workers
            
            # Timeout ì„¤ì •
            if hasattr(scraper, 'timeout'):
                scraper.timeout = scraping_config.get('EXTRACTION_TIMEOUT', 15)
            
            # Extraction strategies ì„¤ì •
            if hasattr(scraper, 'extraction_strategies'):
                strategies = []
                
                if scraping_config.get('ENABLE_DOM_EXTRACTION', True):
                    for strategy in scraper.extraction_strategies:
                        if 'DOMExtractionStrategy' in strategy.__class__.__name__:
                            strategies.append(strategy)
                            
                if scraping_config.get('ENABLE_TEXT_EXTRACTION', True):
                    for strategy in scraper.extraction_strategies:
                        if 'TextExtractionStrategy' in strategy.__class__.__name__:
                            strategies.append(strategy)
                            
                if scraping_config.get('ENABLE_COORD_EXTRACTION', True):
                    for strategy in scraper.extraction_strategies:
                        if 'CoordinateExtractionStrategy' in strategy.__class__.__name__:
                            strategies.append(strategy)
                
                if strategies:
                    scraper.extraction_strategies = strategies
            
            # Politeness delay ì„¤ì •
            if hasattr(scraper, '_search_product_async'):
                # ê¸°ì¡´ ë©”ì„œë“œ ìˆ˜ì •í•˜ì§€ ì•Šê³  ì„¤ì • ì ìš©
                original_method = scraper._search_product_async
                politeness_delay = scraping_config.get('POLITENESS_DELAY', 1500) / 1000  # ms â†’ ì´ˆ
                
                async def patched_method(query, max_items=50, reference_price=None):
                    # politeness_delay í™•ì¸ ë¡œê¹…
                    scraper.logger.debug(f"Using politeness delay of {politeness_delay} seconds")
                    # ì›ë˜ ë©”ì„œë“œ í˜¸ì¶œ
                    result = await original_method(query, max_items, reference_price)
                    return result
                
                scraper._search_product_async = patched_method
    
    def process_file(self, input_file: str) -> Tuple[Optional[str], Optional[str]]:
        """
        ì…ë ¥ ì—‘ì…€ íŒŒì¼ ì²˜ë¦¬ ë° ë³´ê³ ì„œ ìƒì„±
        
        Args:
            input_file: ì…ë ¥ ì—‘ì…€ íŒŒì¼ ê²½ë¡œ
            
        Returns:
            (ê²°ê³¼ íŒŒì¼ ê²½ë¡œ, ì˜¤ë¥˜ ë©”ì‹œì§€)
        """
        try:
            start_time = datetime.now()
            self.logger.info(f"Processing started at: {start_time.strftime('%Y-%m-%d %H:%M:%S')}")
            
            # íŒŒì¼ ì¡´ì¬ í™•ì¸
            if not os.path.exists(input_file):
                error_msg = f"Input file not found: {input_file}"
                self.logger.error(error_msg)
                return None, error_msg
            
            # ì…ë ¥ íŒŒì¼ ì½ê¸°
            try:
                df = self.excel_manager.read_excel_file(input_file)
                
                if df.empty:
                    error_msg = "No data found in input file"
                    self.logger.error(error_msg)
                    return None, error_msg
                    
                total_items = len(df)
                self.logger.info(f"Loaded {total_items} items from {input_file}")
                
                # ë°ì´í„° ì •ì œ
                df = self.data_cleaner.clean_dataframe(df)
                
            except Exception as e:
                self.logger.error(f"Failed to read input file: {str(e)}", exc_info=True)
                return None, f"Failed to read input file: {str(e)}"
            
            # ëŒ€ìš©ëŸ‰ íŒŒì¼ ë¶„í•  ì²˜ë¦¬
            if self.file_splitter.needs_splitting(df):
                try:
                    split_files = self.file_splitter.split_input_file(df, input_file)
                    self.logger.info(f"Input file split into {len(split_files)} files")
                    
                    # ê° ë¶„í•  íŒŒì¼ ì²˜ë¦¬
                    result_files = []
                    for split_file in split_files:
                        result_file, _ = self._process_single_file(split_file)
                        if result_file:
                            result_files.append(result_file)
                    
                    # ê²°ê³¼ ë³‘í•©
                    if len(result_files) > 1:
                        merged_result = self.file_splitter.merge_result_files(result_files, input_file)
                        return merged_result, None
                    
                    return result_files[0] if result_files else None, None
                    
                except Exception as e:
                    self.logger.error(f"Error splitting input file: {str(e)}", exc_info=True)
                    # ë‹¨ì¼ íŒŒì¼ë¡œ ì²˜ë¦¬
                    self.logger.info("Falling back to processing as a single file")
                    return self._process_single_file(input_file)
            else:
                # ë‹¨ì¼ íŒŒì¼ ì²˜ë¦¬
                return self._process_single_file(input_file)
                
        except Exception as e:
            self.logger.error(f"Error in process_file: {str(e)}", exc_info=True)
            return None, str(e)
    
    def _process_single_file(self, input_file: str) -> Tuple[Optional[str], Optional[str]]:
        """ë‹¨ì¼ ì…ë ¥ íŒŒì¼ ì²˜ë¦¬"""
        try:
            start_time = datetime.now()
            
            # ì—‘ì…€ íŒŒì¼ ì½ê¸°
            df = self.excel_manager.read_excel_file(input_file)
            total_items = len(df)
            
            # ë°ì´í„° ì •ì œ
            df = self.data_cleaner.clean_dataframe(df)
            
            # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬
            results = []
            processed_count = 0
            
            for i in range(0, total_items, self.batch_size):
                batch = df.iloc[i:i + self.batch_size]
                batch_futures = []
                
                # ê° í–‰ì— ëŒ€í•´ Product ìƒì„± ë° ì²˜ë¦¬ ì‹œì‘
                for _, row in batch.iterrows():
                    product = self.product_factory.create_product_from_row(row)
                    if product:  # ìœ íš¨í•œ ì œí’ˆë§Œ ì²˜ë¦¬
                        future = self.executor.submit(self._process_single_product, product)
                        batch_futures.append((product, future))
                
                # ë°°ì¹˜ ì™„ë£Œ ëŒ€ê¸°
                for product, future in batch_futures:
                    try:
                        result = future.result(timeout=300)  # 5ë¶„ íƒ€ì„ì•„ì›ƒ
                        results.append(result)
                    except Exception as e:
                        self.logger.error(
                            f"Error processing product {product.id}: {str(e)}", 
                            exc_info=True
                        )
                        # ì‹¤íŒ¨í•œ ê²°ê³¼ë„ ì¶”ê°€ (ìˆœì„œ ìœ ì§€)
                        results.append(ProcessingResult(
                            source_product=product,
                            error=str(e)
                        ))
                    
                    # ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸
                    processed_count += 1
                    progress_percent = int((processed_count / total_items) * 100)
                    if self.progress_callback:
                        self.progress_callback(progress_percent)
                    if processed_count % 10 == 0 or processed_count == total_items:
                        self.logger.info(f"Progress: {processed_count}/{total_items} ({progress_percent}%)")
            
            # ê²°ê³¼ ë³´ê³ ì„œ ìƒì„±
            output_file = self.excel_manager.generate_enhanced_output(results, input_file)
            
            # ì²˜ë¦¬ ì™„ë£Œ ë¡œê¹…
            end_time = datetime.now()
            processing_time = end_time - start_time
            self.logger.info(f"Processing finished at: {end_time.strftime('%Y-%m-%d %H:%M:%S')}")
            self.logger.info(f"Total processing time: {processing_time}")
            
            return output_file, None
            
        except Exception as e:
            self.logger.error(f"Error in _process_single_file: {str(e)}", exc_info=True)
            return None, str(e)
    
    def _process_single_product(self, product: Product) -> ProcessingResult:
        """ë‹¨ì¼ ì œí’ˆ ì²˜ë¦¬"""
        return self.process_product(product)
    
    def process_product(self, product: Product) -> ProcessingResult:
        """
        ë‹¨ì¼ ì œí’ˆì˜ ë§¤ì¹­ ì²˜ë¦¬
        
        Args:
            product: ì²˜ë¦¬í•  Product ê°ì²´
            
        Returns:
            ProductResult ê°ì²´
        """
        self.logger.info(f"Processing product: {product.name} (ID: {product.id})")
        processing_result = ProcessingResult(source_product=product)
        
        # ê³ ë ¤ê¸°í”„íŠ¸ ë§¤ì¹­ ê²€ìƒ‰
        try:
            koryo_matches = self.koryo_scraper.search_product(product.name)
            self.logger.debug(f"Found {len(koryo_matches)} Koryo matches for {product.name}")
            
            # ë§¤ì¹­ ê²°ê³¼ ê³„ì‚°
            for match in koryo_matches:
                match_result = self._calculate_match_similarities(product, match)
                processing_result.koryo_matches.append(match_result)
            
            # ìµœì  ë§¤ì¹­ ì°¾ê¸°
            processing_result.best_koryo_match = self._find_best_match(processing_result.koryo_matches)
            
            if processing_result.best_koryo_match:
                self.logger.info(
                    f"Best Koryo match for {product.name}: "
                    f"{processing_result.best_koryo_match.matched_product.name} "
                    f"({processing_result.best_koryo_match.combined_similarity:.2f})"
                )
        except Exception as e:
            self.logger.error(f"Error finding Koryo matches for {product.name}: {str(e)}", exc_info=True)
            processing_result.error = f"Koryo search error: {str(e)}"
        
        # ë„¤ì´ë²„ ë§¤ì¹­ ê²€ìƒ‰
        try:
            naver_matches = self._safe_naver_search(product.name)
            self.logger.debug(f"Found {len(naver_matches)} Naver matches for {product.name}")
            
            # ë§¤ì¹­ ê²°ê³¼ ê³„ì‚°
            for match in naver_matches:
                try:
                    match_result = self._calculate_match_similarities(product, match)
                    processing_result.naver_matches.append(match_result)
                except Exception as match_err:
                    self.logger.warning(f"Error calculating similarities for {match.name}: {str(match_err)}")
                    continue
            
            # ìµœì  ë§¤ì¹­ ì°¾ê¸°
            processing_result.best_naver_match = self._find_best_match(processing_result.naver_matches)
            
            if processing_result.best_naver_match:
                self.logger.info(
                    f"Best Naver match for {product.name}: "
                    f"{processing_result.best_naver_match.matched_product.name} "
                    f"({processing_result.best_naver_match.combined_similarity:.2f})"
                )
        except Exception as e:
            self.logger.error(f"Error finding Naver matches for {product.name}: {str(e)}", exc_info=True)
            if not processing_result.error:  # ì´ì „ ì˜¤ë¥˜ê°€ ì—†ì„ ê²½ìš°ë§Œ ì„¤ì •
                processing_result.error = f"Naver search error: {str(e)}"
        
        # ë°ì´í„° ê²€ì¦ ë° ë¹„ì–´ìˆëŠ” í•„ë“œ ì²˜ë¦¬
        self._ensure_valid_result(processing_result)
        
        return processing_result
    
    def _ensure_valid_result(self, result: ProcessingResult) -> None:
        """ê²°ê³¼ ë°ì´í„°ê°€ ìœ íš¨í•œì§€ í™•ì¸í•˜ê³  í•„ìš”í•œ ê¸°ë³¸ê°’ì„ ì„¤ì •"""
        # ì†ŒìŠ¤ ì œí’ˆ ë°ì´í„° ê²€ì¦
        if not hasattr(result.source_product, 'original_input_data') or result.source_product.original_input_data is None:
            result.source_product.original_input_data = {}
        
        # í•„ìˆ˜ í•„ë“œ ì¡´ì¬ í™•ì¸
        required_fields = ['êµ¬ë¶„', 'ë‹´ë‹¹ì', 'ì—…ì²´ëª…', 'ì—…ì²´ì½”ë“œ', 'ìƒí’ˆCode', 'ì¤‘ë¶„ë¥˜ì¹´í…Œê³ ë¦¬', 'ìƒí’ˆëª…', 
                          'ê¸°ë³¸ìˆ˜ëŸ‰(1)', 'íŒë§¤ë‹¨ê°€(Ví¬í•¨)', 'ë³¸ì‚¬ìƒí’ˆë§í¬']
        
        for field in required_fields:
            if field not in result.source_product.original_input_data:
                result.source_product.original_input_data[field] = ''
            
        # ì¤‘ìš” í•„ë“œ ìœ íš¨ì„± ê²€ì‚¬
        if 'Code' in result.source_product.original_input_data and not result.source_product.original_input_data.get('ìƒí’ˆCode'):
            result.source_product.original_input_data['ìƒí’ˆCode'] = result.source_product.original_input_data['Code']
        
        if 'ì—…ì²´ì½”ë“œ' in result.source_product.original_input_data and not result.source_product.original_input_data['ì—…ì²´ì½”ë“œ']:
            if hasattr(result.source_product, 'product_code') and result.source_product.product_code:
                result.source_product.original_input_data['ì—…ì²´ì½”ë“œ'] = result.source_product.product_code
    
    def _safe_naver_search(self, query: str) -> List[Product]:
        """ì•ˆì „í•˜ê²Œ ë„¤ì´ë²„ ê²€ìƒ‰ ì‹¤í–‰"""
        products = []
        original_query = query
        
        # ê²€ìƒ‰ ë³€í˜• ì‹œë„ë“¤ (ì›ë˜ ê²€ìƒ‰ì–´, ë‹¨ì–´ ìˆ˜ ì¤„ì´ê¸°)
        queries_to_try = []
        
        # ì›ë˜ ê²€ìƒ‰ì–´ ì¶”ê°€
        queries_to_try.append(original_query)
        
        # ê²€ìƒ‰ì–´ì—ì„œ íŠ¹ìˆ˜ë¬¸ìì™€ ë¶ˆí•„ìš”í•œ ì •ë³´ ì œê±° (ì˜ˆ: ê·œê²©, ìˆ˜ëŸ‰ ì •ë³´)
        cleaned_query = re.sub(r'[^\w\s]', ' ', original_query)
        cleaned_query = re.sub(r'\s+', ' ', cleaned_query).strip()
        if cleaned_query != original_query:
            queries_to_try.append(cleaned_query)
        
        # ë‹¨ì–´ ìˆ˜ ì¤„ì´ê¸° (ê¸´ ê²€ìƒ‰ì–´ì˜ ê²½ìš° ì•ìª½ 3-4 ë‹¨ì–´ë§Œ ì‚¬ìš©)
        words = cleaned_query.split()
        if len(words) > 4:
            shortened_query = ' '.join(words[:4])
            queries_to_try.append(shortened_query)
        elif len(words) > 3:
            shortened_query = ' '.join(words[:3])
            queries_to_try.append(shortened_query)
        
        # ê° ê²€ìƒ‰ì–´ ë³€í˜•ìœ¼ë¡œ ì‹œë„
        max_attempts = 3  # ìµœëŒ€ ë³€í˜• íšŸìˆ˜ ì œí•œ
        
        for attempt, current_query in enumerate(queries_to_try[:max_attempts]):
            if attempt > 0:
                self.logger.info(f"ğŸ” ê²€ìƒ‰ì–´ ë³€í˜• ì‹œë„ {attempt}: '{current_query}'")
            
            try:
                # ê¸°ë³¸ í˜¸ì¶œ ì‹œë„
                current_products = self.naver_crawler.search_product(current_query)
                
                if current_products:
                    products.extend(current_products)
                    self.logger.info(f"âœ… ê²€ìƒ‰ ì„±ê³µ: '{current_query}'ì—ì„œ {len(current_products)}ê°œ ìƒí’ˆ ë°œê²¬")
                    
                    # ì¶©ë¶„í•œ ê²°ê³¼ë¥¼ ì°¾ì•˜ìœ¼ë©´ ë” ì´ìƒ ì‹œë„í•˜ì§€ ì•ŠìŒ
                    if len(products) >= 10:
                        break
                else:
                    self.logger.warning(f"âš ï¸ '{current_query}'ì— ëŒ€í•œ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
            
            except TypeError as e:
                # ì¸ì ì˜¤ë¥˜ ì‹œ ëŒ€ì²´ í˜¸ì¶œ
                self.logger.warning(f"Type error during Naver search: {str(e)}. Trying alternative method.")
                try:
                    # ì§ì ‘ ë‚´ë¶€ ê²€ìƒ‰ ë¡œì§ í˜¸ì¶œ
                    if hasattr(self.naver_crawler, '_search_product_logic'):
                        current_products = self.naver_crawler._search_product_logic(current_query, 50, None)
                        if current_products:
                            products.extend(current_products)
                            self.logger.info(f"âœ… ëŒ€ì²´ ê²€ìƒ‰ ì„±ê³µ: '{current_query}'ì—ì„œ {len(current_products)}ê°œ ìƒí’ˆ ë°œê²¬")
                    elif hasattr(self.naver_crawler, '_search_product_async'):
                        # ë¹„ë™ê¸° í˜¸ì¶œ ì²˜ë¦¬
                        import asyncio
                        loop = asyncio.new_event_loop()
                        asyncio.set_event_loop(loop)
                        try:
                            current_products = loop.run_until_complete(
                                self.naver_crawler._search_product_async(current_query, 50, None)
                            )
                            if current_products:
                                products.extend(current_products)
                                self.logger.info(f"âœ… ë¹„ë™ê¸° ê²€ìƒ‰ ì„±ê³µ: '{current_query}'ì—ì„œ {len(current_products)}ê°œ ìƒí’ˆ ë°œê²¬")
                        finally:
                            loop.close()
                except Exception as inner_e:
                    self.logger.error(f"Alternative Naver search failed: {str(inner_e)}", exc_info=True)
            
            except Exception as e:
                self.logger.error(f"Error in Naver search: {str(e)}", exc_info=True)
        
        # ì¤‘ë³µ ì œê±°
        unique_products = {}
        for product in products:
            if product.id not in unique_products:
                unique_products[product.id] = product
        
        # ê²°ê³¼ê°€ ì—†ê±°ë‚˜ ì¶©ë¶„í•˜ì§€ ì•Šì€ ê²½ìš° ë‹¤ë¥¸ ê²€ìƒ‰ ì—”ì§„ì´ë‚˜ ë°©ë²•ì„ ì‹œë„í•  ìˆ˜ ìˆìŒ
        if not unique_products:
            self.logger.warning(f"âŒ ëª¨ë“  ê²€ìƒ‰ ì‹œë„ í›„ì—ë„ '{original_query}'ì— ëŒ€í•œ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
        else:
            self.logger.info(f"ğŸ¯ '{original_query}'ì— ëŒ€í•´ ì´ {len(unique_products)}ê°œì˜ ê³ ìœ  ìƒí’ˆ ë°œê²¬")
            
        return list(unique_products.values())
    
    def _calculate_match_similarities(self, source_product: Product, matched_product: Product) -> MatchResult:
        """ë‘ ì œí’ˆ ê°„ì˜ ìœ ì‚¬ë„ ê³„ì‚°"""
        # í…ìŠ¤íŠ¸ ìœ ì‚¬ë„
        text_sim = self.text_matcher.calculate_similarity(
            source_product.name,
            matched_product.name
        )
        
        # ì´ë¯¸ì§€ ìœ ì‚¬ë„ ê³„ì‚° ê°œì„ 
        # ì†ŒìŠ¤ ì´ë¯¸ì§€ URL í™•ì¸
        source_image_url = source_product.original_input_data.get('ë³¸ì‚¬ ì´ë¯¸ì§€', '')
        if not source_image_url and source_product.image_url:
            source_image_url = source_product.image_url
            
        # ë§¤ì¹˜ëœ ì´ë¯¸ì§€ URL í™•ì¸
        match_image_url = matched_product.image_url or ''
        
        # ì´ë¯¸ì§€ ìœ ì‚¬ë„ ê³„ì‚°
        image_sim = 0.0  # ê¸°ë³¸ê°’
        if source_image_url and match_image_url:
            self.logger.debug(f"Calculating image similarity between: {source_image_url} and {match_image_url}")
            image_sim = self.image_matcher.calculate_similarity(source_image_url, match_image_url)
        else:
            # ì†ŒìŠ¤ì™€ ë§¤ì¹˜ ì œê³µìë¥¼ ëª…í™•íˆ í‘œì‹œí•˜ì—¬ ë¡œê¹…
            if not source_image_url and match_image_url:
                # ì›ë³¸ ì œí’ˆì˜ ì´ë¯¸ì§€ê°€ ì—†ì§€ë§Œ ë§¤ì¹­ëœ ì œí’ˆ(ë„¤ì´ë²„/ê³ ë ¤)ì€ ì´ë¯¸ì§€ê°€ ìˆëŠ” ê²½ìš°
                source_name = source_product.name
                match_source = "ë„¤ì´ë²„" if matched_product.source == 'naver_api' else "ê³ ë ¤ê¸°í”„íŠ¸"
                
                # ì´ëª¨ì§€ë¥¼ ì‚¬ìš©í•˜ì—¬ êµ¬ë¶„í•˜ê¸° ì‰½ê²Œ ë§Œë“¦
                self.logger.warning(f"âš ï¸ ì›ë³¸ ì œí’ˆ '{source_name}'ì— ì´ë¯¸ì§€ê°€ ì—†ì§€ë§Œ, {match_source}ì—ì„œ ë§¤ì¹­ëœ ì œí’ˆì—ëŠ” ì´ë¯¸ì§€ê°€ ìˆìŠµë‹ˆë‹¤: {match_image_url}")
                
                # ì›ë³¸ ì œí’ˆì— ì´ë¯¸ì§€ê°€ ì—†ëŠ” ê²½ìš°ì—ë„ í’ˆì§ˆ ë¶„ì„ì— í¬ê²Œ ì˜í–¥ì„ ì£¼ì§€ ì•Šë„ë¡ ì´ë¯¸ì§€ ìœ ì‚¬ë„ì— ê¸°ë³¸ê°’ ë¶€ì—¬
                # 0.0ì€ ë„ˆë¬´ ë‚®ì•„ ì „ì²´ ë§¤ì¹­ ì ìˆ˜ë¥¼ ë‚®ì¶œ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì¤‘ê°„ ì ìˆ˜ì¸ 0.5 ë¶€ì—¬
                if matched_product.source == 'naver_api':
                    # ë„¤ì´ë²„ API ê²°ê³¼ëŠ” í•­ìƒ ì´ë¯¸ì§€ë¥¼ í¬í•¨í•˜ë¯€ë¡œ ë” ë†’ì€ ê¸°ë³¸ê°’ ì‚¬ìš©
                    image_sim = 0.5
            else:
                # ê¸°íƒ€ ì´ë¯¸ì§€ ëˆ„ë½ ì¼€ì´ìŠ¤ (ë‘˜ ë‹¤ ì—†ê±°ë‚˜ ë§¤ì¹­ëœ ì œí’ˆ ì´ë¯¸ì§€ê°€ ì—†ëŠ” ê²½ìš°)
                self.logger.warning(f"Missing image URL for similarity calculation: source={bool(source_image_url)}, match={bool(match_image_url)}")
        
        # í†µí•© ìœ ì‚¬ë„
        combined_sim = self.multimodal_matcher.calculate_similarity(
            text_sim,
            image_sim
        )
        
        self.logger.debug(
            f"  Match candidate {matched_product.name} ({matched_product.source}): "
            f"Txt={text_sim:.2f}, Img={image_sim:.2f}, Comb={combined_sim:.2f}"
        )
        
        # ê°€ê²© ì°¨ì´ ê³„ì‚°
        price_diff = 0.0
        price_diff_percent = 0.0
        source_price = source_product.price
        
        if source_price and source_price > 0 and isinstance(matched_product.price, (int, float)):
            price_diff = matched_product.price - source_price
            price_diff_percent = (price_diff / source_price) * 100 if source_price != 0 else 0
        
        return MatchResult(
            source_product=source_product,
            matched_product=matched_product,
            text_similarity=text_sim,
            image_similarity=image_sim,
            combined_similarity=combined_sim,
            price_difference=price_diff,
            price_difference_percent=price_diff_percent
        )
    
    def _find_best_match(self, matches: List[MatchResult]) -> Optional[MatchResult]:
        """
        ë§¤ì¹­ ê²°ê³¼ ì¤‘ ìµœì ì˜ ê²°ê³¼ ì„ íƒ
        
        ë§¤ë‰´ì–¼ ìš”êµ¬ì‚¬í•­:
        1. ìƒí’ˆ ì´ë¦„ìœ¼ë¡œ ê²€ìƒ‰í•˜ì—¬ ë™ì¼ ìƒí’ˆ ì°¾ê¸°
        2. ì´ë¯¸ì§€ë¡œ ì œí’ˆ ë¹„êµ (ì´ë¯¸ì§€ ë¹„êµê°€ ì–´ë ¤ìš´ ê²½ìš° ê·œê²© í™•ì¸)
        3. ë™ì¼ ìƒí’ˆìœ¼ë¡œ íŒë‹¨ë˜ë©´ ê°€ì¥ ë‚®ì€ ê°€ê²©ì˜ ìƒí’ˆ ì„ íƒ
        """
        if not matches:
            return None
        
        # ì„ê³„ê°’ ì„¤ì •
        text_threshold = self.config['MATCHING'].get('TEXT_SIMILARITY_THRESHOLD', 0.65)
        image_threshold = self.config['MATCHING'].get('IMAGE_SIMILARITY_THRESHOLD', 0.3)
        
        # ì—„ê²©í•œ ë§¤ì¹­: ì„ê³„ê°’ ì´ìƒì¸ ë§¤ì¹­ë§Œ í•„í„°ë§ (ë™ì¼ ìƒí’ˆìœ¼ë¡œ ê°„ì£¼)
        valid_matches = [
            m for m in matches
            if m.text_similarity >= text_threshold
            and (
                # ì´ë¯¸ì§€ê°€ ìˆëŠ” ê²½ìš°ëŠ” ì´ë¯¸ì§€ ìœ ì‚¬ë„ ê²€ì‚¬
                (m.image_similarity >= image_threshold) or
                # ì´ë¯¸ì§€ê°€ ì—†ì§€ë§Œ í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ê°€ ë§¤ìš° ë†’ì€ ê²½ìš°(0.85 ì´ìƒ) ì˜ˆì™¸ í—ˆìš©
                (m.image_similarity == 0 and m.text_similarity >= 0.85)
            )
        ]
        
        # ì„ê³„ê°’ì„ í†µê³¼í•œ ë§¤ì¹­ì´ ìˆìœ¼ë©´ ê·¸ ì¤‘ì—ì„œ ìµœì €ê°€ ì„ íƒ
        if valid_matches:
            best_match = min(valid_matches, key=lambda x: x.matched_product.price if x.matched_product.price > 0 else float('inf'))
            self.logger.info(f"ğŸ’¯ ì—„ê²©í•œ ì„ê³„ê°’ì„ í†µê³¼í•œ ìµœì ì˜ ë§¤ì¹­: {best_match.matched_product.name} (í…ìŠ¤íŠ¸ ìœ ì‚¬ë„: {best_match.text_similarity:.2f}, ì´ë¯¸ì§€ ìœ ì‚¬ë„: {best_match.image_similarity:.2f}, ê°€ê²©: {best_match.matched_product.price})")
            return best_match
        
        # ì„ê³„ê°’ì„ í†µê³¼í•œ ë§¤ì¹­ì´ ì—†ìœ¼ë©´ ë” ë‚®ì€ ì„ê³„ê°’ ì‹œë„
        relaxed_text_threshold = text_threshold * 0.75  # 25% ë‚®ì€ ì„ê³„ê°’
        relaxed_matches = [
            m for m in matches
            if m.text_similarity >= relaxed_text_threshold
        ]
        
        if relaxed_matches:
            # ë‚®ì€ ì„ê³„ê°’ì—ì„œëŠ” í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ì™€ ê°€ê²©ì„ ë™ì‹œì— ê³ ë ¤í•´ ê°€ì¥ ì í•©í•œ ë§¤ì¹­ ì„ íƒ
            # í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ë¡œ ì •ë ¬ í›„ ìƒìœ„ 3ê°œ ì¤‘ì—ì„œ ìµœì €ê°€ ì„ íƒ
            top_matches = sorted(relaxed_matches, key=lambda x: x.text_similarity, reverse=True)[:3]
            best_match = min(top_matches, key=lambda x: x.matched_product.price if x.matched_product.price > 0 else float('inf'))
            
            self.logger.warning(f"âš ï¸ ë‚®ì€ ì„ê³„ê°’ìœ¼ë¡œ ë§¤ì¹­ ë°œê²¬: {best_match.matched_product.name} (í…ìŠ¤íŠ¸ ìœ ì‚¬ë„: {best_match.text_similarity:.2f}, ì´ë¯¸ì§€ ìœ ì‚¬ë„: {best_match.image_similarity:.2f}, ê°€ê²©: {best_match.matched_product.price})")
            return best_match
        
        # ëª¨ë“  ì„ê³„ê°’ì—ì„œ ë§¤ì¹­ì„ ì°¾ì§€ ëª»í•œ ê²½ìš°, ëª¨ë“  ë§¤ì¹­ ì¤‘ ê°€ì¥ ìœ ì‚¬í•œ í•˜ë‚˜ ë°˜í™˜ (ë§¤ìš° ìœ ì—°í•œ ëŒ€ì•ˆ)
        if matches:
            # í†µí•© ìœ ì‚¬ë„ë¡œ ì •ë ¬í•´ ê°€ì¥ ë†’ì€ í•˜ë‚˜ ì„ íƒ
            best_match = max(matches, key=lambda x: x.combined_similarity)
            self.logger.warning(f"â— ëª¨ë“  ì„ê³„ê°’ ì‹¤íŒ¨, ê°€ì¥ ìœ ì‚¬í•œ ì œí’ˆ ì„ íƒ: {best_match.matched_product.name} (í†µí•© ìœ ì‚¬ë„: {best_match.combined_similarity:.2f})")
            return best_match
        
        return None

    def process_files(self, input_files: List[str], output_dir: str = None, limit: int = None) -> List[str]:
        """ì—¬ëŸ¬ íŒŒì¼ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤."""
        try:
            if not input_files:
                self.logger.error("ì…ë ¥ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
                return []
            
            # ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì •
            if output_dir:
                os.makedirs(output_dir, exist_ok=True)
            
            output_files = []
            for input_file in input_files:
                try:
                    # ì œí•œëœ ìˆ˜ì˜ ìƒí’ˆë§Œ ì²˜ë¦¬
                    if limit:
                        output_file = self._process_limited_file(input_file, output_dir, limit)
                    else:
                        output_file = self._process_single_file(input_file, output_dir)
                    
                    if output_file:
                        output_files.append(output_file)
                    
                except Exception as e:
                    self.logger.error(f"íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {input_file}, ì˜¤ë¥˜: {str(e)}", exc_info=True)
                    continue
                
            return output_files
        
        except Exception as e:
            self.logger.error(f"íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}", exc_info=True)
            return []

    def _process_limited_file(self, input_file: str, output_dir: str = None, limit: int = 10) -> Optional[str]:
        """ì œí•œëœ ìˆ˜ì˜ ìƒí’ˆë§Œ ì²˜ë¦¬í•©ë‹ˆë‹¤."""
        try:
            self.logger.info(f"íŒŒì¼ ì²˜ë¦¬ ì‹œì‘: {input_file} (ìµœëŒ€ {limit}ê°œ ìƒí’ˆ)")
            
            # Excel íŒŒì¼ ì½ê¸°
            df = pd.read_excel(input_file)
            if df.empty:
                self.logger.error(f"íŒŒì¼ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤: {input_file}")
                return None
            
            # ë°ì´í„° ì •ì œ
            df = self._clean_data(df)
            
            # ì œí•œëœ ìˆ˜ì˜ ìƒí’ˆë§Œ ì„ íƒ
            if len(df) > limit:
                self.logger.info(f"ì „ì²´ {len(df)}ê°œ ì¤‘ {limit}ê°œ ìƒí’ˆë§Œ ì²˜ë¦¬í•©ë‹ˆë‹¤.")
                df = df.head(limit)
            
            # ê²°ê³¼ ì €ì¥ì„ ìœ„í•œ ë¦¬ìŠ¤íŠ¸
            results = []
            
            # ê° ìƒí’ˆ ì²˜ë¦¬
            for idx, row in df.iterrows():
                try:
                    # ìƒí’ˆ ì²˜ë¦¬
                    result = self._process_single_product(row)
                    if result:
                        results.append(result)
                    
                except Exception as e:
                    self.logger.error(f"ìƒí’ˆ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ (í–‰ {idx+1}): {str(e)}", exc_info=True)
                    continue
            
            # ê²°ê³¼ê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ ì¶œë ¥ íŒŒì¼ ìƒì„±
            if results:
                # ì¶œë ¥ íŒŒì¼ëª… ìƒì„±
                if output_dir:
                    base_name = os.path.basename(input_file)
                    output_file = os.path.join(output_dir, f"{os.path.splitext(base_name)[0]}-result.xlsx")
                else:
                    output_file = f"{os.path.splitext(input_file)[0]}-result.xlsx"
                
                # Excel íŒŒì¼ ìƒì„±
                self.excel_manager.generate_enhanced_output(results, output_file)
                
                self.logger.info(f"íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ: {output_file}")
                return output_file
            
            return None
        
        except Exception as e:
            self.logger.error(f"íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}", exc_info=True)
            return None 