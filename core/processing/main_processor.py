import logging
import os
import re
from concurrent.futures import ThreadPoolExecutor
from datetime import datetime
from typing import Dict, List, Optional, Tuple

import pandas as pd

from utils.caching import FileCache

from ..data_models import MatchResult, ProcessingResult, Product
from ..matching.image_matcher import ImageMatcher
from ..matching.multimodal_matcher import MultiModalMatcher
from ..matching.text_matcher import TextMatcher
from ..scraping.haeoeum_scraper import HaeoeumScraper
from ..scraping.koryo_scraper import KoryoScraper
from ..scraping.naver_crawler import NaverShoppingCrawler
from .data_cleaner import DataCleaner
from .excel_manager import ExcelManager
from .file_splitter import FileSplitter
from .product_factory import ProductFactory


class ProductProcessor:
    """ì œí’ˆ ë°ì´í„° ì²˜ë¦¬ë¥¼ ìœ„í•œ ë©”ì¸ í´ë˜ìŠ¤"""

    def __init__(self, config: Dict):
        """
        ì œí’ˆ í”„ë¡œì„¸ì„œ ì´ˆê¸°í™”

        Args:
            config: ì• í”Œë¦¬ì¼€ì´ì…˜ ì„¤ì •
        """
        self.config = config
        self.logger = logging.getLogger(__name__)
        self.progress_callback = None  # ì§„í–‰ìƒí™© ì½œë°± ì´ˆê¸°í™”
        self._is_running = True  # Add running flag
        self._init_components()

        # ë°°ì¹˜ ì²˜ë¦¬ ì„¤ì •
        self.batch_size = config["PROCESSING"].get("BATCH_SIZE", 10)

    def stop_processing(self):
        """Stop the processing gracefully."""
        self.logger.info("Processing stop requested.")
        self._is_running = False

    def _init_components(self):
        """í•„ìš”í•œ ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”"""
        # ìºì‹œ ì´ˆê¸°í™”
        self.cache = FileCache(
            cache_dir=self.config["PATHS"]["CACHE_DIR"],
            duration_seconds=self.config["PROCESSING"]["CACHE_DURATION"],
            max_size_mb=self.config["PROCESSING"].get("CACHE_MAX_SIZE_MB", 1024),
            enable_compression=self.config["PROCESSING"].get(
                "ENABLE_COMPRESSION", False
            ),
            compression_level=self.config["PROCESSING"].get("COMPRESSION_LEVEL", 6),
        )

        # ë§¤ì¹­ ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.text_matcher = TextMatcher(cache=self.cache)

        # ì´ë¯¸ì§€ ì²˜ë¦¬ ìµœëŒ€ í•´ìƒë„ ì„¤ì • ì¶”ê°€
        if "MAX_IMAGE_DIMENSION" not in self.config["MATCHING"]:
            self.config["MATCHING"]["MAX_IMAGE_DIMENSION"] = 256
            self.logger.info(f"Setting default MAX_IMAGE_DIMENSION to 256px")

        self.image_matcher = ImageMatcher(
            cache=self.cache,
            similarity_threshold=self.config["MATCHING"]["IMAGE_SIMILARITY_THRESHOLD"],
        )

        self.multimodal_matcher = MultiModalMatcher(
            text_weight=self.config["MATCHING"]["TEXT_WEIGHT"],
            image_weight=self.config["MATCHING"]["IMAGE_WEIGHT"],
            text_matcher=self.text_matcher,
            image_matcher=self.image_matcher,
            similarity_threshold=self.config["MATCHING"].get(
                "TEXT_SIMILARITY_THRESHOLD", 0.75
            ),
        )

        # ìŠ¤í¬ë˜í¼ ì´ˆê¸°í™”
        self.koryo_scraper = KoryoScraper(
            max_retries=self.config["PROCESSING"]["MAX_RETRIES"],
            cache=self.cache,
            timeout=self.config["PROCESSING"].get("REQUEST_TIMEOUT", 30),
        )

        # í”„ë¡ì‹œ ì‚¬ìš© ì—¬ë¶€ í™•ì¸
        use_proxies = False
        if (
            "NETWORK" in self.config
            and self.config["NETWORK"].get("USE_PROXIES") == "True"
        ):
            use_proxies = True
            self.logger.info("í”„ë¡ì‹œ ì‚¬ìš© ëª¨ë“œë¡œ ë„¤ì´ë²„ í¬ë¡¤ëŸ¬ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.")

        self.naver_crawler = NaverShoppingCrawler(
            max_retries=self.config["PROCESSING"]["MAX_RETRIES"],
            cache=self.cache,
            timeout=self.config["PROCESSING"].get("REQUEST_TIMEOUT", 30),
        )

        # ìŠ¤í¬ë˜í¼ ì„¤ì • ì ìš©
        scraping_config = self.config.get("SCRAPING", {})
        if scraping_config:
            self._configure_scrapers(scraping_config)

        # ìµœì í™”ëœ ë³‘ë ¬ ì²˜ë¦¬ ì„¤ì •
        # ì‹œìŠ¤í…œ ì½”ì–´ ìˆ˜ ê¸°ë°˜ max_workers ìë™ ì„¤ì •
        import multiprocessing

        cpu_count = multiprocessing.cpu_count()
        default_workers = max(4, min(cpu_count * 2, 16))  # ìµœì†Œ 4, ìµœëŒ€ 16 ì›Œì»¤

        # ìŠ¤ë ˆë“œí’€ ì´ˆê¸°í™”
        max_workers = self.config["PROCESSING"].get("MAX_WORKERS", default_workers)
        self.logger.info(f"ë³‘ë ¬ ì²˜ë¦¬ ì›Œì»¤ ìˆ˜: {max_workers} (CPU ì½”ì–´: {cpu_count})")
        self.executor = ThreadPoolExecutor(
            max_workers=max_workers, thread_name_prefix="ProductProcessor"
        )

        # ë°°ì¹˜ í¬ê¸° ìµœì í™”
        default_batch = min(
            20, max(5, cpu_count)
        )  # ì½”ì–´ ìˆ˜ì— ë§ê²Œ ì¡°ì •, ìµœì†Œ 5, ìµœëŒ€ 20
        self.batch_size = self.config["PROCESSING"].get("BATCH_SIZE", default_batch)
        self.logger.info(f"ë°°ì¹˜ í¬ê¸° ì„¤ì •: {self.batch_size}")

        # ìœ í‹¸ë¦¬í‹° ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.excel_manager = ExcelManager(self.config, self.logger)
        self.data_cleaner = DataCleaner(self.config, self.logger)
        self.product_factory = ProductFactory(
            self.config, self.logger, self.data_cleaner
        )
        self.file_splitter = FileSplitter(self.config, self.logger)

    def _configure_scrapers(self, scraping_config: Dict):
        """ìŠ¤í¬ë˜í¼ ì„¤ì • ì ìš©"""
        scrapers = [self.koryo_scraper, self.naver_crawler]

        for scraper in scrapers:
            # Max workers ì„¤ì •
            max_workers = scraping_config.get("MAX_CONCURRENT_REQUESTS", 5)
            if hasattr(scraper, "executor") and hasattr(
                scraper.executor, "_max_workers"
            ):
                scraper.executor._max_workers = max_workers

            # Timeout ì„¤ì •
            if hasattr(scraper, "timeout"):
                scraper.timeout = scraping_config.get("EXTRACTION_TIMEOUT", 15)

            # Extraction strategies ì„¤ì •
            if hasattr(scraper, "extraction_strategies"):
                strategies = []

                if scraping_config.get("ENABLE_DOM_EXTRACTION", True):
                    for strategy in scraper.extraction_strategies:
                        if "DOMExtractionStrategy" in strategy.__class__.__name__:
                            strategies.append(strategy)

                if scraping_config.get("ENABLE_TEXT_EXTRACTION", True):
                    for strategy in scraper.extraction_strategies:
                        if "TextExtractionStrategy" in strategy.__class__.__name__:
                            strategies.append(strategy)

                if scraping_config.get("ENABLE_COORD_EXTRACTION", True):
                    for strategy in scraper.extraction_strategies:
                        if (
                            "CoordinateExtractionStrategy"
                            in strategy.__class__.__name__
                        ):
                            strategies.append(strategy)

                if strategies:
                    scraper.extraction_strategies = strategies

            # Politeness delay ì„¤ì •
            if hasattr(scraper, "_search_product_async"):
                # ê¸°ì¡´ ë©”ì„œë“œ ìˆ˜ì •í•˜ì§€ ì•Šê³  ì„¤ì • ì ìš©
                original_method = scraper._search_product_async
                politeness_delay = (
                    scraping_config.get("POLITENESS_DELAY", 1500) / 1000
                )  # ms â†’ ì´ˆ

                async def patched_method(query, max_items=50, reference_price=None):
                    # politeness_delay í™•ì¸ ë¡œê¹…
                    scraper.logger.debug(
                        f"Using politeness delay of {politeness_delay} seconds"
                    )
                    # ì›ë˜ ë©”ì„œë“œ í˜¸ì¶œ
                    result = await original_method(query, max_items, reference_price)
                    return result

                scraper._search_product_async = patched_method

    def process_file(self, input_file: str) -> Tuple[Optional[str], Optional[str]]:
        """
        ì…ë ¥ ì—‘ì…€ íŒŒì¼ ì²˜ë¦¬ ë° ë³´ê³ ì„œ ìƒì„±

        Args:
            input_file: ì…ë ¥ ì—‘ì…€ íŒŒì¼ ê²½ë¡œ

        Returns:
            (ê²°ê³¼ íŒŒì¼ ê²½ë¡œ, ì˜¤ë¥˜ ë©”ì‹œì§€)
        """
        try:
            start_time = datetime.now()
            self.logger.info(
                f"Processing started at: {start_time.strftime('%Y-%m-%d %H:%M:%S')}"
            )

            # íŒŒì¼ ì¡´ì¬ í™•ì¸
            if not os.path.exists(input_file):
                error_msg = f"Input file not found: {input_file}"
                self.logger.error(error_msg)
                return None, error_msg

            # ì—‘ì…€ ì „ì²˜ë¦¬ ì‘ì—… (XLS -> XLSX ë³€í™˜ ë° í•„ìš”í•œ ì»¬ëŸ¼ ì¶”ê°€)
            input_file = self.process_excel_functionality(input_file)

            # ì…ë ¥ íŒŒì¼ ì½ê¸°
            try:
                df = self.excel_manager.read_excel_file(input_file)

                if df.empty:
                    error_msg = "No data found in input file"
                    self.logger.error(error_msg)
                    return None, error_msg

                total_items = len(df)
                self.logger.info(f"Loaded {total_items} items from {input_file}")

                # ë°ì´í„° ì •ì œ
                df = self.data_cleaner.clean_dataframe(df)

            except Exception as e:
                self.logger.error(f"Failed to read input file: {str(e)}", exc_info=True)
                return None, f"Failed to read input file: {str(e)}"

            # ëŒ€ìš©ëŸ‰ íŒŒì¼ ë¶„í•  ì²˜ë¦¬
            if self.file_splitter.needs_splitting(df):
                try:
                    split_files = self.file_splitter.split_input_file(df, input_file)
                    self.logger.info(f"Input file split into {len(split_files)} files")

                    # ê° ë¶„í•  íŒŒì¼ ì²˜ë¦¬
                    result_files = []
                    for split_file in split_files:
                        result_file, _ = self._process_single_file(split_file)
                        if result_file:
                            result_files.append(result_file)

                    # ê²°ê³¼ ë³‘í•©
                    if len(result_files) > 1:
                        merged_result = self.file_splitter.merge_result_files(
                            result_files, input_file
                        )
                        return merged_result, None

                    return result_files[0] if result_files else None, None

                except Exception as e:
                    self.logger.error(
                        f"Error splitting input file: {str(e)}", exc_info=True
                    )
                    # ë‹¨ì¼ íŒŒì¼ë¡œ ì²˜ë¦¬
                    self.logger.info("Falling back to processing as a single file")
                    return self._process_single_file(input_file)
            else:
                # ë‹¨ì¼ íŒŒì¼ ì²˜ë¦¬
                return self._process_single_file(input_file)

        except Exception as e:
            self.logger.error(f"Error in process_file: {str(e)}", exc_info=True)
            return None, str(e)

    def process_excel_functionality(self, input_file: str) -> str:
        """
        ì—‘ì…€ íŒŒì¼ì— ëŒ€í•œ ì „ì²˜ë¦¬ ì‘ì—…ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.

        Args:
            input_file: ì…ë ¥ ì—‘ì…€ íŒŒì¼ ê²½ë¡œ

        Returns:
            str: ì²˜ë¦¬ëœ íŒŒì¼ ê²½ë¡œ (ë³€ê²½ì´ ìˆëŠ” ê²½ìš° ìƒˆ íŒŒì¼ ê²½ë¡œ, ì—†ìœ¼ë©´ ì›ë³¸ ê²½ë¡œ)
        """
        try:
            input_dir = os.path.dirname(input_file)
            input_ext = os.path.splitext(input_file)[1].lower()

            # 1. XLS -> XLSX ë³€í™˜ (í™•ì¥ìê°€ .xlsì¸ ê²½ìš°)
            if input_ext == ".xls":
                self.logger.info(f"XLS íŒŒì¼ ê°ì§€: {input_file}")
                xlsx_file = self.excel_manager.convert_xls_to_xlsx(input_dir)
                if xlsx_file:
                    self.logger.info(f"XLS íŒŒì¼ì´ XLSXë¡œ ë³€í™˜ë˜ì—ˆìŠµë‹ˆë‹¤: {xlsx_file}")
                    input_file = xlsx_file
                else:
                    self.logger.warning(
                        "XLS íŒŒì¼ ë³€í™˜ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ì›ë³¸ íŒŒì¼ì„ ì‚¬ìš©í•©ë‹ˆë‹¤."
                    )

            # 2. @ ê¸°í˜¸ ì œê±°
            input_file = self.excel_manager.remove_at_symbol(input_file)

            # 3. í•„ìš”í•œ ì»¬ëŸ¼ í™•ì¸ ë° ì¶”ê°€
            self.excel_manager.check_excel_file(input_file)

            return input_file

        except Exception as e:
            self.logger.error(f"ì—‘ì…€ ì „ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}", exc_info=True)
            return input_file  # ì˜¤ë¥˜ ë°œìƒ ì‹œ ì›ë³¸ íŒŒì¼ ì‚¬ìš©

    def post_process_output_file(self, output_file: str) -> str:
        """
        ì¶œë ¥ ì—‘ì…€ íŒŒì¼ì— ëŒ€í•œ í›„ì²˜ë¦¬ ì‘ì—…ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.

        Args:
            output_file: ì²˜ë¦¬ëœ ì—‘ì…€ íŒŒì¼ ê²½ë¡œ

        Returns:
            str: ìµœì¢… ì¶œë ¥ íŒŒì¼ ê²½ë¡œ
        """
        try:
            # 1. í•˜ì´í¼ë§í¬ ì¶”ê°€
            linked_file = self.excel_manager.add_hyperlinks_to_excel(output_file)

            # 2. ê°€ê²© ì°¨ì´ê°€ ìˆëŠ” í•­ëª©ë§Œ í•„í„°ë§
            filtered_file = self.excel_manager.filter_excel_by_price_diff(linked_file)

            # 3. í¬ë§·íŒ… ì ìš©
            self.excel_manager.apply_formatting_to_excel(filtered_file)

            return filtered_file

        except Exception as e:
            self.logger.error(f"ì—‘ì…€ í›„ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}", exc_info=True)
            return output_file

    def _process_single_file(
        self, input_file: str, output_dir: Optional[str] = None
    ) -> Optional[str]:
        """ë‹¨ì¼ ì…ë ¥ íŒŒì¼ ì²˜ë¦¬"""
        # Reset running flag at the start of processing a file
        self._is_running = True
        try:
            start_time = datetime.now()
            self.logger.info(
                f"Processing file: {input_file}, started at: {start_time.strftime('%Y-%m-%d %H:%M:%S')}"
            )

            # ì—‘ì…€ íŒŒì¼ ì½ê¸°
            df = self.excel_manager.read_excel_file(input_file)
            if df.empty:
                self.logger.warning(f"Input file is empty: {input_file}")
                return None

            total_items = len(df)
            self.logger.info(f"Loaded {total_items} items from {input_file}")

            # ë°ì´í„° ì •ì œ
            df = self.data_cleaner.clean_dataframe(df)

            # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬
            results = []
            processed_count = 0

            # Emit initial progress
            if self.progress_callback:
                try:
                    self.progress_callback(0, total_items)
                except Exception as cb_e:
                    self.logger.error(f"Error in initial progress callback: {cb_e}")

            for i in range(0, total_items, self.batch_size):
                # Check if stopped before starting a new batch
                if not self._is_running:
                    self.logger.warning("Processing stopped by request (batch loop).")
                    break

                batch = df.iloc[i : i + self.batch_size]
                batch_futures = []

                # ê° í–‰ì— ëŒ€í•´ Product ìƒì„± ë° ì²˜ë¦¬ ì‹œì‘
                for _, row in batch.iterrows():
                     # Check if stopped before submitting a new task
                    if not self._is_running:
                        self.logger.warning("Processing stopped by request (task submission loop).")
                        break # Break inner loop

                    product = self.product_factory.create_product_from_row(row)
                    if product:  # ìœ íš¨í•œ ì œí’ˆë§Œ ì²˜ë¦¬
                        future = self.executor.submit(
                            self._process_single_product, product
                        )
                        batch_futures.append((product, future))
                
                # Check again if stopped after submitting tasks for the batch
                if not self._is_running:
                    break # Break outer loop if stopped during task submission

                # ë°°ì¹˜ ì™„ë£Œ ëŒ€ê¸°
                for product, future in batch_futures:
                    # Check if stopped before getting result (allows faster stop)
                    if not self._is_running:
                        self.logger.warning("Processing stopped by request (result loop).")
                        # Attempt to cancel pending future if possible
                        if not future.done():
                            future.cancel()
                        continue # Skip getting result and updating progress for this item

                    try:
                        result = future.result(timeout=300)  # 5ë¶„ íƒ€ì„ì•„ì›ƒ
                        results.append(result)
                    except Exception as e:
                        self.logger.error(
                            f"Error processing product {product.id}: {str(e)}",
                            exc_info=True,
                        )
                        # ì‹¤íŒ¨í•œ ê²°ê³¼ë„ ì¶”ê°€ (ìˆœì„œ ìœ ì§€)
                        results.append(
                            ProcessingResult(source_product=product, error=str(e))
                        )
                    finally:
                        # Ensure progress is updated even if there was an error or stop request
                        processed_count += 1
                        if self.progress_callback:
                             try:
                                # Use processed_count and total_items
                                self.progress_callback(processed_count, total_items)
                             except Exception as cb_e:
                                 self.logger.error(f"Error in progress callback: {cb_e}")

                        if processed_count % 10 == 0 or processed_count == total_items:
                            progress_percent = int((processed_count / total_items) * 100) if total_items > 0 else 0
                            self.logger.info(
                                f"Progress: {processed_count}/{total_items} ({progress_percent}%)"
                            )
                
                # Check if stopped after processing the batch
                if not self._is_running:
                    break # Break outer loop if stopped

            # Check if processing was stopped before generating output
            if not self._is_running:
                self.logger.warning("Processing was stopped. Skipping output file generation.")
                return None

            # ê²°ê³¼ ë³´ê³ ì„œ ìƒì„± (only if results exist and processing wasn't stopped)
            if results:
                output_file = self.excel_manager.generate_enhanced_output(
                    results, input_file, output_dir # Pass output_dir
                )

                # í›„ì²˜ë¦¬ ì‘ì—… ìˆ˜í–‰ (í•˜ì´í¼ë§í¬, í•„í„°ë§ ë“±)
                output_file = self.post_process_output_file(output_file)

                # ì²˜ë¦¬ ì™„ë£Œ ë¡œê¹…
                end_time = datetime.now()
                processing_time = end_time - start_time
                self.logger.info(
                    f"Processing finished at: {end_time.strftime('%Y-%m-%d %H:%M:%S')}"
                )
                self.logger.info(f"Total processing time: {processing_time}")

                return output_file, None # Return tuple
            else:
                 self.logger.info("No results generated, possibly due to empty input or errors.")
                 return None, "No results generated" # Return tuple indicating no output

        except Exception as e:
            self.logger.error(f"Error in _process_single_file: {str(e)}", exc_info=True)
            return None, str(e) # Return tuple

    def _process_single_product(self, product: Product) -> ProcessingResult:
        """ë‹¨ì¼ ì œí’ˆ ì²˜ë¦¬"""
        return self.process_product(product)

    def process_product(self, product: Product) -> ProcessingResult:
        """
        ë‹¨ì¼ ì œí’ˆì˜ ë§¤ì¹­ ì²˜ë¦¬

        Args:
            product: ì²˜ë¦¬í•  Product ê°ì²´

        Returns:
            ProductResult ê°ì²´
        """
        self.logger.info(f"Processing product: {product.name} (ID: {product.id})")
        self.logger.info(f"Product source: {product.source}")

        # í•´ì˜¤ë¦„ê¸°í”„íŠ¸ ìƒí’ˆ í™•ì¸
        if product.source.lower() != "haeoreum":
            self.logger.warning(
                f"Product source is not Haeoreum Gift: {product.source}"
            )
            # í•´ì˜¤ë¦„ ì†ŒìŠ¤ë¡œ ì„¤ì •
            product.source = "haeoreum"
            self.logger.info(f"Reset product source to Haeoreum Gift")

        processing_result = ProcessingResult(source_product=product)

        # í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ì„ê³„ê°’ ì„¤ì •
        text_threshold = self.config["MATCHING"].get("TEXT_SIMILARITY_THRESHOLD", 0.65)
        # í…ìŠ¤íŠ¸ ì´ˆê¸° í•„í„°ë§ì„ ìœ„í•œ ë‚®ì€ ì„ê³„ê°’ (ì„±ëŠ¥ ê°œì„ ì„ ìœ„í•œ í•„í„°ë§ìš©)
        initial_text_threshold = text_threshold * 0.7  # ì„ê³„ê°’ì˜ 70%

        # ê³ ë ¤ê¸°í”„íŠ¸ ë§¤ì¹­ ê²€ìƒ‰
        try:
            self.logger.info(f"Searching Koryo Gift for: {product.name}")
            koryo_matches = self.koryo_scraper.search_product(product.name)

            if not koryo_matches:
                self.logger.info(
                    f"âŒ ê³ ë ¤ê¸°í”„íŠ¸ì—ì„œ '{product.name}' ìƒí’ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŒ"
                )
            else:
                self.logger.debug(
                    f"âœ… ê³ ë ¤ê¸°í”„íŠ¸ì—ì„œ '{product.name}' ìƒí’ˆ {len(koryo_matches)}ê°œ ë°œê²¬"
                )

                # 1ë‹¨ê³„: í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ë§Œ ë¨¼ì € ê³„ì‚°í•˜ì—¬ í›„ë³´êµ° ì¶”ë¦¬ê¸°
                text_filtered_matches = []
                for match in koryo_matches:
                    text_sim = self.text_matcher.calculate_similarity(
                        product.name, match.name
                    )
                    if text_sim >= initial_text_threshold:
                        text_filtered_matches.append((match, text_sim))

                self.logger.info(
                    f"ğŸ” í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ë¡œ {len(text_filtered_matches)}/{len(koryo_matches)}ê°œ í›„ë³´ ì¶”ë ¤ëƒ„ (ì„ê³„ê°’: {initial_text_threshold:.2f})"
                )

                # 2ë‹¨ê³„: í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ê°€ ë†’ì€ í›„ë³´ë“¤ì— ëŒ€í•´ì„œë§Œ ì´ë¯¸ì§€ ìœ ì‚¬ë„ ê³„ì‚°
                for match, text_sim in text_filtered_matches:
                    # ê¸°ë³¸ MatchResult ìƒì„±
                    match_result = MatchResult(
                        source_product=product,
                        matched_product=match,
                        text_similarity=text_sim,
                        image_similarity=0.0,
                        combined_similarity=0.0,
                        price_difference=0.0,
                        price_difference_percent=0.0,
                    )

                    # ì´ë¯¸ì§€ ìœ ì‚¬ë„ ë° ê°€ê²© ì°¨ì´ ê³„ì‚°
                    self._calculate_image_similarity_and_price(match_result)

                    # ê²°ê³¼ ì¶”ê°€
                    processing_result.koryo_matches.append(match_result)

            # ìµœì  ë§¤ì¹­ ì°¾ê¸°
            processing_result.best_koryo_match = self._find_best_match(
                processing_result.koryo_matches
            )

            if processing_result.best_koryo_match:
                if processing_result.best_koryo_match.image_similarity > 0:
                    self.logger.info(
                        f"âœ… ê³ ë ¤ê¸°í”„íŠ¸ ë§¤ì¹­ (ì´ë¯¸ì§€ í¬í•¨): {processing_result.best_koryo_match.matched_product.name}"
                    )
                else:
                    self.logger.info(
                        f"ğŸ“ ê³ ë ¤ê¸°í”„íŠ¸ ë§¤ì¹­ (í…ìŠ¤íŠ¸ë§Œ): {processing_result.best_koryo_match.matched_product.name}"
                    )

        except Exception as e:
            self.logger.error(
                f"Error finding Koryo matches for {product.name}: {str(e)}",
                exc_info=True,
            )
            processing_result.error = f"Koryo search error: {str(e)}"

        # ë„¤ì´ë²„ ë§¤ì¹­ ê²€ìƒ‰
        try:
            self.logger.info(f"Searching Naver for: {product.name}")
            naver_matches = self._safe_naver_search(product.name)

            if not naver_matches:
                self.logger.info(f"âŒ ë„¤ì´ë²„ì—ì„œ '{product.name}' ìƒí’ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            elif len(naver_matches) == 1 and getattr(naver_matches[0], 'id', '') == "no_match":
                self.logger.info(
                    f"âŒ ë„¤ì´ë²„ì—ì„œ '{product.name}' ìƒí’ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŒ (no_match ë°˜í™˜)"
                )
            else:
                self.logger.debug(
                    f"âœ… ë„¤ì´ë²„ì—ì„œ '{product.name}' ìƒí’ˆ {len(naver_matches)}ê°œ ë°œê²¬"
                )

                # 'no_match' ë”ë¯¸ ìƒí’ˆ ì œì™¸ - ì´ ë¶€ë¶„ì˜ í•„í„°ë§ ë¡œì§ë„ ìˆ˜ì •
                real_matches = []
                for m in naver_matches:
                    if not hasattr(m, 'id') or m.id != "no_match":
                        real_matches.append(m)
                
                # ì‹¤ì œ ê²€ìƒ‰ ê²°ê³¼ê°€ ìˆëŠ”ì§€ ë‹¤ì‹œ í™•ì¸
                if real_matches:
                    # 1ë‹¨ê³„: í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ë§Œ ë¨¼ì € ê³„ì‚°í•˜ì—¬ í›„ë³´êµ° ì¶”ë¦¬ê¸°
                    text_filtered_matches = []
                    for match in real_matches:
                        text_sim = self.text_matcher.calculate_similarity(
                            product.name, match.name
                        )
                        if text_sim >= initial_text_threshold:
                            text_filtered_matches.append((match, text_sim))

                    self.logger.info(
                        f"ğŸ” í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ë¡œ {len(text_filtered_matches)}/{len(real_matches)}ê°œ í›„ë³´ ì¶”ë ¤ëƒ„ (ì„ê³„ê°’: {initial_text_threshold:.2f})"
                    )

                    # 2ë‹¨ê³„: í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ê°€ ë†’ì€ í›„ë³´ë“¤ì— ëŒ€í•´ì„œë§Œ ì´ë¯¸ì§€ ìœ ì‚¬ë„ ê³„ì‚°
                    for match, text_sim in text_filtered_matches:
                        # ê¸°ë³¸ MatchResult ìƒì„±
                        match_result = MatchResult(
                            source_product=product,
                            matched_product=match,
                            text_similarity=text_sim,
                            image_similarity=0.0,
                            combined_similarity=0.0,
                            price_difference=0.0,
                            price_difference_percent=0.0,
                        )

                        # ì´ë¯¸ì§€ ìœ ì‚¬ë„ ë° ê°€ê²© ì°¨ì´ ê³„ì‚°
                        self._calculate_image_similarity_and_price(match_result)

                        # ê²°ê³¼ ì¶”ê°€
                        processing_result.naver_matches.append(match_result)
                else:
                    self.logger.info(f"âŒ ë„¤ì´ë²„ì—ì„œ '{product.name}' ìƒí’ˆì˜ ìœ íš¨í•œ ë§¤ì¹˜ê°€ ì—†ìŒ")

            # ìµœì  ë§¤ì¹­ ì°¾ê¸°
            processing_result.best_naver_match = self._find_best_match(
                processing_result.naver_matches
            )

            if processing_result.best_naver_match:
                if processing_result.best_naver_match.image_similarity > 0:
                    self.logger.info(
                        f"âœ… ë„¤ì´ë²„ ë§¤ì¹­ (ì´ë¯¸ì§€ í¬í•¨): {processing_result.best_naver_match.matched_product.name}"
                    )
                else:
                    self.logger.info(
                        f"ğŸ“ ë„¤ì´ë²„ ë§¤ì¹­ (í…ìŠ¤íŠ¸ë§Œ): {processing_result.best_naver_match.matched_product.name}"
                    )

        except Exception as e:
            self.logger.error(
                f"Error finding Naver matches for {product.name}: {str(e)}",
                exc_info=True,
            )
            if not processing_result.error:  # ì´ì „ ì˜¤ë¥˜ê°€ ì—†ì„ ê²½ìš°ë§Œ ì„¤ì •
                processing_result.error = f"Naver search error: {str(e)}"

        # ë°ì´í„° ê²€ì¦ ë° ë¹„ì–´ìˆëŠ” í•„ë“œ ì²˜ë¦¬
        self._ensure_valid_result(processing_result)

        return processing_result

    def _calculate_image_similarity_and_price(self, match_result: MatchResult) -> None:
        """
        ì´ë¯¸ì§€ ìœ ì‚¬ë„ì™€ ê°€ê²© ì°¨ì´ë¥¼ ê³„ì‚°í•˜ê³  MatchResultì— ì„¤ì •

        Args:
            match_result: ì—…ë°ì´íŠ¸í•  MatchResult ê°ì²´
        """
        source_product = match_result.source_product
        matched_product = match_result.matched_product

        # ì†ŒìŠ¤ ì´ë¯¸ì§€ URL í™•ì¸
        source_image_url = ""

        # í•´ì˜¤ë¦„ ê¸°í”„íŠ¸(ì›ë³¸)ì˜ ì´ë¯¸ì§€ë¥¼ í™•ì¸
        if source_product.source == "haeoreum":
            # 1. ë³¸ì‚¬ ì´ë¯¸ì§€ í•„ë“œì—ì„œ ë¨¼ì € í™•ì¸
            if (
                "ë³¸ì‚¬ ì´ë¯¸ì§€" in source_product.original_input_data
                and source_product.original_input_data["ë³¸ì‚¬ ì´ë¯¸ì§€"]
            ):
                source_image_url = str(
                    source_product.original_input_data["ë³¸ì‚¬ ì´ë¯¸ì§€"]
                ).strip()

            # 2. ì´ë¯¸ì§€ URL ì†ì„± í™•ì¸
            if not source_image_url and source_product.image_url:
                source_image_url = source_product.image_url

            # 3. ë³¸ì‚¬ìƒí’ˆë§í¬ í™•ì¸ (ì´ë¯¸ì§€ê°€ ì—†ëŠ” ê²½ìš° ì§ì ‘ ìŠ¤í¬ë˜í•‘í•˜ì—¬ ì´ë¯¸ì§€ ì¶”ì¶œ)
            if (
                not source_image_url
                and "ë³¸ì‚¬ìƒí’ˆë§í¬" in source_product.original_input_data
                and source_product.original_input_data["ë³¸ì‚¬ìƒí’ˆë§í¬"]
            ):
                product_link = str(
                    source_product.original_input_data["ë³¸ì‚¬ìƒí’ˆë§í¬"]
                ).strip()
                if (
                    product_link
                    and "jclgift.com" in product_link
                    and "p_idx=" in product_link
                ):
                    self.logger.info(
                        f"No image URL found, scraping from product link: {product_link}"
                    )

                    try:
                        # URLì—ì„œ p_idx íŒŒë¼ë¯¸í„° ì¶”ì¶œ
                        import re

                        from ..scraping.haeoeum_scraper import HaeoeumScraper

                        p_idx_match = re.search(r"p_idx=(\d+)", product_link)
                        if p_idx_match:
                            p_idx = p_idx_match.group(1)

                            # í•´ì˜¤ë¦„ ìŠ¤í¬ë˜í¼ë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ ì¶”ì¶œ
                            scraper = HaeoeumScraper(cache=self.cache)
                            scraped_product = scraper.get_product(p_idx)

                            if scraped_product and scraped_product.image_url:
                                source_image_url = scraped_product.image_url
                                self.logger.info(
                                    f"Successfully extracted image URL: {source_image_url}"
                                )

                                # ê²°ê³¼ë¬¼ì„ ì†ŒìŠ¤ ì œí’ˆì— ì €ì¥ (í–¥í›„ ì‚¬ìš©)
                                source_product.image_url = source_image_url
                                source_product.original_input_data["ë³¸ì‚¬ ì´ë¯¸ì§€"] = source_image_url

                                # ì´ë¯¸ì§€ ê°¤ëŸ¬ë¦¬ë„ ìˆìœ¼ë©´ ì €ì¥
                                if scraped_product.image_gallery:
                                    source_product.image_gallery = (
                                        scraped_product.image_gallery
                                    )
                                    self.logger.info(
                                        f"Added {len(scraped_product.image_gallery)} images to gallery"
                                    )
                            else:
                                self.logger.warning(
                                    f"Failed to extract image from {product_link}"
                                )
                    except Exception as e:
                        self.logger.error(
                            f"Error scraping image from product link: {str(e)}"
                        )
        else:
            # ë‹¤ë¥¸ ì†ŒìŠ¤ì˜ ê²½ìš° ë‹¨ìˆœíˆ image_url ì†ì„± ì‚¬ìš©
            source_image_url = source_product.image_url

        # ë§¤ì¹˜ëœ ì´ë¯¸ì§€ URL í™•ì¸
        match_image_url = matched_product.image_url or ""

        # ë§¤ì¹­ ì†ŒìŠ¤ í™•ì¸ (ë„¤ì´ë²„ ë˜ëŠ” ê³ ë ¤ê¸°í”„íŠ¸)
        match_source = (
            "ë„¤ì´ë²„" if matched_product.source == "naver_api" else "ê³ ë ¤ê¸°í”„íŠ¸"
        )

        # ì´ë¯¸ì§€ ìœ ì‚¬ë„ ê³„ì‚°
        image_sim = 0.0  # ê¸°ë³¸ê°’

        # ì–‘ìª½ ëª¨ë‘ ì´ë¯¸ì§€ê°€ ìˆëŠ” ê²½ìš°: ì´ë¯¸ì§€ ìœ ì‚¬ë„ ê³„ì‚°
        if source_image_url and match_image_url:
            # ìºì‹œ í‚¤ ìƒì„± (URL í•´ì‹œ ì‚¬ìš©)
            import hashlib

            cache_key = f"img_sim_{hashlib.md5((source_image_url + match_image_url).encode()).hexdigest()}"

            # ìºì‹œì—ì„œ ìœ ì‚¬ë„ ê°’ í™•ì¸
            cached_sim = self.cache.get(cache_key)
            if cached_sim is not None:
                image_sim = cached_sim
                self.logger.debug(f"ğŸ”„ ìºì‹œì—ì„œ ì´ë¯¸ì§€ ìœ ì‚¬ë„ ë¡œë“œ: {image_sim:.2f}")
            else:
                self.logger.debug(
                    f"ğŸ–¼ï¸ ì´ë¯¸ì§€ ìœ ì‚¬ë„ ê³„ì‚°: {source_image_url} <-> {match_image_url}"
                )
                try:
                    # ì´ë¯¸ì§€ í•´ìƒë„ ì¶•ì†Œ ì„¤ì • (ë¹ ë¥¸ ë¹„êµë¥¼ ìœ„í•¨)
                    max_size = self.config["MATCHING"].get("MAX_IMAGE_DIMENSION", 256)

                    # ì´ë¯¸ì§€ ìœ ì‚¬ë„ ê³„ì‚° ì‹œ í•´ìƒë„ ì œí•œ
                    image_sim = self.image_matcher.calculate_similarity(
                        source_image_url, match_image_url, max_dimension=max_size
                    )

                    # ê²°ê³¼ ìºì‹± (1ì¼ ìœ ì§€)
                    self.cache.set(cache_key, image_sim, ttl=86400)

                    self.logger.debug(f"  ì´ë¯¸ì§€ ìœ ì‚¬ë„ ê²°ê³¼: {image_sim:.2f}")
                except Exception as e:
                    self.logger.warning(f"ì´ë¯¸ì§€ ìœ ì‚¬ë„ ê³„ì‚° ì¤‘ ì˜¤ë¥˜: {str(e)}")
                    # ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ë³¸ê°’ ì‚¬ìš©
                    image_sim = 0.0
        # í•´ì˜¤ë¦„ ì œí’ˆì— ì´ë¯¸ì§€ê°€ ì—†ê³  ë§¤ì¹­ëœ ì œí’ˆì— ì´ë¯¸ì§€ê°€ ìˆëŠ” ê²½ìš°
        elif not source_image_url and match_image_url:
            self.logger.warning(
                f"âš ï¸ í•´ì˜¤ë¦„ ì œí’ˆ '{source_product.name}'ì— ì´ë¯¸ì§€ê°€ ì—†ìœ¼ë‚˜, {match_source}ì— ì´ë¯¸ì§€ ìˆìŒ"
            )
            # ë„¤ì´ë²„ëŠ” ì´ë¯¸ì§€ê°€ ìˆì„ í™•ë¥ ì´ ë†’ìœ¼ë¯€ë¡œ ë” ë†’ì€ ê¸°ë³¸ê°’ ë¶€ì—¬
            if matched_product.source == "naver_api":
                image_sim = 0.5
            else:
                image_sim = 0.4
        # ë§¤ì¹­ëœ ì œí’ˆì— ì´ë¯¸ì§€ê°€ ì—†ëŠ” ê²½ìš°
        elif source_image_url and not match_image_url:
            self.logger.warning(
                f"âš ï¸ í•´ì˜¤ë¦„ ì œí’ˆ '{source_product.name}'ì— ì´ë¯¸ì§€ê°€ ìˆìœ¼ë‚˜, {match_source}ì— ì´ë¯¸ì§€ ì—†ìŒ"
            )
            # í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ê°€ ë†’ìœ¼ë©´ ì´ë¯¸ì§€ ìœ ì‚¬ë„ ê¸°ë³¸ê°’ ë¶€ì—¬ (0.3)
            if match_result.text_similarity >= 0.75:
                image_sim = 0.3
        # ë‘˜ ë‹¤ ì´ë¯¸ì§€ê°€ ì—†ëŠ” ê²½ìš°
        else:
            self.logger.warning(
                f"âš ï¸ ë‘ ì œí’ˆ ëª¨ë‘ ì´ë¯¸ì§€ ì—†ìŒ: í•´ì˜¤ë¦„ '{source_product.name}' <-> {match_source} '{matched_product.name}'"
            )
            # í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ê°€ ë§¤ìš° ë†’ì€ ê²½ìš°ë§Œ ì•½ê°„ì˜ ê¸°ë³¸ê°’ ë¶€ì—¬
            if match_result.text_similarity >= 0.85:
                image_sim = 0.2

        # ì´ë¯¸ì§€ ìœ ì‚¬ë„ ì„¤ì •
        match_result.image_similarity = image_sim

        # í†µí•© ìœ ì‚¬ë„ ê³„ì‚° ë° ì„¤ì •
        match_result.combined_similarity = self.multimodal_matcher.calculate_similarity(
            match_result.text_similarity, match_result.image_similarity
        )

        # ê°€ê²© ì°¨ì´ ê³„ì‚°
        price_diff = 0.0
        price_diff_percent = 0.0
        source_price = source_product.price

        if (
            source_price
            and source_price > 0
            and isinstance(matched_product.price, (int, float))
        ):
            price_diff = matched_product.price - source_price
            price_diff_percent = (
                (price_diff / source_price) * 100 if source_price != 0 else 0
            )

        # ê°€ê²© ì°¨ì´ ì„¤ì •
        match_result.price_difference = price_diff
        match_result.price_difference_percent = price_diff_percent

        self.logger.debug(
            f"  Match candidate {matched_product.name} ({matched_product.source}): "
            f"Txt={match_result.text_similarity:.2f}, Img={match_result.image_similarity:.2f}, "
            f"Comb={match_result.combined_similarity:.2f}, Price diff: {price_diff}"
        )

    def _ensure_valid_result(self, result: ProcessingResult) -> None:
        """ê²°ê³¼ ë°ì´í„°ê°€ ìœ íš¨í•œì§€ í™•ì¸í•˜ê³  í•„ìš”í•œ ê¸°ë³¸ê°’ì„ ì„¤ì •"""
        # ì†ŒìŠ¤ ì œí’ˆ ë°ì´í„° ê²€ì¦
        if (
            not hasattr(result.source_product, "original_input_data")
            or result.source_product.original_input_data is None
        ):
            result.source_product.original_input_data = {}

        # í•„ìˆ˜ í•„ë“œ ì¡´ì¬ í™•ì¸
        required_fields = [
            "êµ¬ë¶„",
            "ë‹´ë‹¹ì",
            "ì—…ì²´ëª…",
            "ì—…ì²´ì½”ë“œ",
            "ìƒí’ˆCode",
            "ì¤‘ë¶„ë¥˜ì¹´í…Œê³ ë¦¬",
            "ìƒí’ˆëª…",
            "ê¸°ë³¸ìˆ˜ëŸ‰(1)",
            "íŒë§¤ë‹¨ê°€(Ví¬í•¨)",
            "ë³¸ì‚¬ìƒí’ˆë§í¬",
        ]

        for field in required_fields:
            if field not in result.source_product.original_input_data:
                result.source_product.original_input_data[field] = ""

        # ì¤‘ìš” í•„ë“œ ìœ íš¨ì„± ê²€ì‚¬
        if (
            "Code" in result.source_product.original_input_data
            and not result.source_product.original_input_data.get("ìƒí’ˆCode")
        ):
            result.source_product.original_input_data["ìƒí’ˆCode"] = (
                result.source_product.original_input_data["Code"]
            )

        if (
            "ì—…ì²´ì½”ë“œ" in result.source_product.original_input_data
            and not result.source_product.original_input_data["ì—…ì²´ì½”ë“œ"]
        ):
            if (
                hasattr(result.source_product, "product_code")
                and result.source_product.product_code
            ):
                result.source_product.original_input_data["ì—…ì²´ì½”ë“œ"] = (
                    result.source_product.product_code
                )

    def _safe_naver_search(self, query: str) -> List[Product]:
        """ì•ˆì „í•˜ê²Œ ë„¤ì´ë²„ ê²€ìƒ‰ ì‹¤í–‰"""
        products = []
        original_query = query

        # ê²€ìƒ‰ ë³€í˜• ì‹œë„ë“¤ (ì›ë˜ ê²€ìƒ‰ì–´, ë‹¨ì–´ ìˆ˜ ì¤„ì´ê¸°)
        queries_to_try = []

        # ì›ë˜ ê²€ìƒ‰ì–´ ì¶”ê°€
        queries_to_try.append(original_query)

        # ê²€ìƒ‰ì–´ì—ì„œ íŠ¹ìˆ˜ë¬¸ìì™€ ë¶ˆí•„ìš”í•œ ì •ë³´ ì œê±° (ì˜ˆ: ê·œê²©, ìˆ˜ëŸ‰ ì •ë³´)
        cleaned_query = re.sub(r"[^\w\s]", " ", original_query)
        cleaned_query = re.sub(r"\s+", " ", cleaned_query).strip()
        if cleaned_query != original_query:
            queries_to_try.append(cleaned_query)

        # ë‹¨ì–´ ìˆ˜ ì¤„ì´ê¸° (ê¸´ ê²€ìƒ‰ì–´ì˜ ê²½ìš° ì•ìª½ 3-4 ë‹¨ì–´ë§Œ ì‚¬ìš©)
        words = cleaned_query.split()
        if len(words) > 4:
            shortened_query = " ".join(words[:4])
            queries_to_try.append(shortened_query)
        elif len(words) > 3:
            shortened_query = " ".join(words[:3])
            queries_to_try.append(shortened_query)

        # ê° ê²€ìƒ‰ì–´ ë³€í˜•ìœ¼ë¡œ ì‹œë„
        max_attempts = 3  # ìµœëŒ€ ë³€í˜• íšŸìˆ˜ ì œí•œ

        for attempt, current_query in enumerate(queries_to_try[:max_attempts]):
            if attempt > 0:
                self.logger.info(f"ğŸ” ê²€ìƒ‰ì–´ ë³€í˜• ì‹œë„ {attempt}: '{current_query}'")

            try:
                # ê¸°ë³¸ í˜¸ì¶œ ì‹œë„
                current_products = self.naver_crawler.search_product(current_query)

                if current_products:
                    products.extend(current_products)
                    self.logger.info(
                        f"âœ… ê²€ìƒ‰ ì„±ê³µ: '{current_query}'ì—ì„œ {len(current_products)}ê°œ ìƒí’ˆ ë°œê²¬"
                    )

                    # ì¶©ë¶„í•œ ê²°ê³¼ë¥¼ ì°¾ì•˜ìœ¼ë©´ ë” ì´ìƒ ì‹œë„í•˜ì§€ ì•ŠìŒ
                    if len(products) >= 10:
                        break
                else:
                    self.logger.warning(f"âš ï¸ '{current_query}'ì— ëŒ€í•œ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")

            except TypeError as e:
                # ì¸ì ì˜¤ë¥˜ ì‹œ ëŒ€ì²´ í˜¸ì¶œ
                self.logger.warning(
                    f"Type error during Naver search: {str(e)}. Trying alternative method."
                )
                try:
                    # ì§ì ‘ ë‚´ë¶€ ê²€ìƒ‰ ë¡œì§ í˜¸ì¶œ
                    if hasattr(self.naver_crawler, "_search_product_logic"):
                        current_products = self.naver_crawler._search_product_logic(
                            current_query, 50, None
                        )
                        if current_products:
                            products.extend(current_products)
                            self.logger.info(
                                f"âœ… ëŒ€ì²´ ê²€ìƒ‰ ì„±ê³µ: '{current_query}'ì—ì„œ {len(current_products)}ê°œ ìƒí’ˆ ë°œê²¬"
                            )
                    elif hasattr(self.naver_crawler, "_search_product_async"):
                        # ë¹„ë™ê¸° í˜¸ì¶œ ì²˜ë¦¬
                        import asyncio

                        loop = asyncio.new_event_loop()
                        asyncio.set_event_loop(loop)
                        try:
                            current_products = loop.run_until_complete(
                                self.naver_crawler._search_product_async(
                                    current_query, 50, None
                                )
                            )
                            if current_products:
                                products.extend(current_products)
                                self.logger.info(
                                    f"âœ… ë¹„ë™ê¸° ê²€ìƒ‰ ì„±ê³µ: '{current_query}'ì—ì„œ {len(current_products)}ê°œ ìƒí’ˆ ë°œê²¬"
                                )
                        finally:
                            loop.close()
                except Exception as inner_e:
                    self.logger.error(
                        f"Alternative Naver search failed: {str(inner_e)}",
                        exc_info=True,
                    )

            except Exception as e:
                self.logger.error(f"Error in Naver search: {str(e)}", exc_info=True)

        # ì¤‘ë³µ ì œê±°
        unique_products = {}
        for product in products:
            if product.id not in unique_products:
                unique_products[product.id] = product

        # ê²°ê³¼ê°€ ì—†ê±°ë‚˜ ì¶©ë¶„í•˜ì§€ ì•Šì€ ê²½ìš° ë‹¤ë¥¸ ê²€ìƒ‰ ì—”ì§„ì´ë‚˜ ë°©ë²•ì„ ì‹œë„í•  ìˆ˜ ìˆìŒ
        if not unique_products:
            self.logger.warning(
                f"âŒ ëª¨ë“  ê²€ìƒ‰ ì‹œë„ í›„ì—ë„ '{original_query}'ì— ëŒ€í•œ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ"
            )
        else:
            self.logger.info(
                f"ğŸ¯ '{original_query}'ì— ëŒ€í•´ ì´ {len(unique_products)}ê°œì˜ ê³ ìœ  ìƒí’ˆ ë°œê²¬"
            )

        return list(unique_products.values())

    def _find_best_match(self, matches: List[MatchResult]) -> Optional[MatchResult]:
        """
        ë§¤ì¹­ ê²°ê³¼ ì¤‘ ìµœì ì˜ ê²°ê³¼ ì„ íƒ

        ë§¤ë‰´ì–¼ ìš”êµ¬ì‚¬í•­:
        1. ìƒí’ˆ ì´ë¦„ìœ¼ë¡œ ê²€ìƒ‰í•˜ì—¬ ë™ì¼ ìƒí’ˆ ì°¾ê¸°
        2. ì´ë¯¸ì§€ë¡œ ì œí’ˆ ë¹„êµ (ì´ë¯¸ì§€ ë¹„êµê°€ ì–´ë ¤ìš´ ê²½ìš° ê·œê²© í™•ì¸)
        3. ë™ì¼ ìƒí’ˆìœ¼ë¡œ íŒë‹¨ë˜ë©´ ê°€ì¥ ë‚®ì€ ê°€ê²©ì˜ ìƒí’ˆ ì„ íƒ
        """
        if not matches:
            self.logger.warning(
                "ğŸš« ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ: í•´ë‹¹ ìƒí’ˆì´ ê³ ë ¤ê¸°í”„íŠ¸/ë„¤ì´ë²„ì— ì¡´ì¬í•˜ì§€ ì•ŠìŒ"
            )
            return None

        # ì´ë¯¸ì§€ê°€ ìˆëŠ” ë§¤ì¹­ê³¼ ì—†ëŠ” ë§¤ì¹­ ë¶„ë¦¬
        matches_with_image = [m for m in matches if m.image_similarity > 0]
        matches_without_image = [m for m in matches if m.image_similarity == 0]

        # ë¡œê¹…
        if not matches_with_image and matches_without_image:
            source_name = matches[0].source_product.name
            match_source = (
                "ë„¤ì´ë²„"
                if matches[0].matched_product.source == "naver_api"
                else "ê³ ë ¤ê¸°í”„íŠ¸"
            )
            self.logger.info(
                f"ğŸ“‹ '{source_name}': {match_source}ì—ì„œ {len(matches)} ë§¤ì¹­ì´ ë°œê²¬ë˜ì—ˆìœ¼ë‚˜ ì´ë¯¸ì§€ë¥¼ ê°€ì§„ ë§¤ì¹­ ì—†ìŒ"
            )

        # ì„ê³„ê°’ ì„¤ì •
        text_threshold = self.config["MATCHING"].get("TEXT_SIMILARITY_THRESHOLD", 0.65)
        image_threshold = self.config["MATCHING"].get("IMAGE_SIMILARITY_THRESHOLD", 0.3)

        # ì—„ê²©í•œ ë§¤ì¹­: ì„ê³„ê°’ ì´ìƒì¸ ë§¤ì¹­ë§Œ í•„í„°ë§ (ë™ì¼ ìƒí’ˆìœ¼ë¡œ ê°„ì£¼)
        valid_matches = [
            m
            for m in matches
            if m.text_similarity >= text_threshold
            and (
                # ì´ë¯¸ì§€ê°€ ìˆëŠ” ê²½ìš°ëŠ” ì´ë¯¸ì§€ ìœ ì‚¬ë„ ê²€ì‚¬
                (m.image_similarity >= image_threshold)
                or
                # ì´ë¯¸ì§€ê°€ ì—†ì§€ë§Œ í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ê°€ ë§¤ìš° ë†’ì€ ê²½ìš°(0.85 ì´ìƒ) ì˜ˆì™¸ í—ˆìš©
                (m.image_similarity == 0 and m.text_similarity >= 0.85)
            )
        ]

        # ì„ê³„ê°’ì„ í†µê³¼í•œ ë§¤ì¹­ì´ ìˆìœ¼ë©´ ê·¸ ì¤‘ì—ì„œ ìµœì €ê°€ ì„ íƒ
        if valid_matches:
            # ì´ë¯¸ì§€ ìˆëŠ” ë§¤ì¹­ê³¼ ì—†ëŠ” ë§¤ì¹­ ì¤‘ ì„ íƒ ìš°ì„ ìˆœìœ„ ê²°ì •
            valid_with_image = [
                m for m in valid_matches if m.image_similarity >= image_threshold
            ]

            if valid_with_image:
                # ì´ë¯¸ì§€ê°€ ìˆëŠ” ë§¤ì¹­ ì¤‘ì—ì„œ ìµœì €ê°€ ì„ íƒ
                best_match = min(
                    valid_with_image,
                    key=lambda x: (
                        x.matched_product.price
                        if x.matched_product.price > 0
                        else float("inf")
                    ),
                )
                self.logger.info(
                    f"ğŸ’¯ ì´ë¯¸ì§€ ë§¤ì¹­ ì„±ê³µ: {best_match.matched_product.name} (í…ìŠ¤íŠ¸ ìœ ì‚¬ë„: {best_match.text_similarity:.2f}, ì´ë¯¸ì§€ ìœ ì‚¬ë„: {best_match.image_similarity:.2f}, ê°€ê²©: {best_match.matched_product.price})"
                )
            else:
                # ì´ë¯¸ì§€ ì—†ì´ í…ìŠ¤íŠ¸ë§Œ ë§¤ì¹­ëœ ê²½ìš°
                best_match = min(
                    valid_matches,
                    key=lambda x: (
                        x.matched_product.price
                        if x.matched_product.price > 0
                        else float("inf")
                    ),
                )
                self.logger.info(
                    f"ğŸ“ í…ìŠ¤íŠ¸ë§Œ ë§¤ì¹­ ì„±ê³µ: {best_match.matched_product.name} (í…ìŠ¤íŠ¸ ìœ ì‚¬ë„: {best_match.text_similarity:.2f}, ê°€ê²©: {best_match.matched_product.price})"
                )
            return best_match

        # ì„ê³„ê°’ì„ í†µê³¼í•œ ë§¤ì¹­ì´ ì—†ìœ¼ë©´ ë” ë‚®ì€ ì„ê³„ê°’ ì‹œë„
        relaxed_text_threshold = text_threshold * 0.75  # 25% ë‚®ì€ ì„ê³„ê°’
        relaxed_matches = [
            m for m in matches if m.text_similarity >= relaxed_text_threshold
        ]

        if relaxed_matches:
            # ë‚®ì€ ì„ê³„ê°’ì—ì„œëŠ” í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ì™€ ê°€ê²©ì„ ë™ì‹œì— ê³ ë ¤í•´ ê°€ì¥ ì í•©í•œ ë§¤ì¹­ ì„ íƒ
            # í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ë¡œ ì •ë ¬ í›„ ìƒìœ„ 3ê°œ ì¤‘ì—ì„œ ìµœì €ê°€ ì„ íƒ
            top_matches = sorted(
                relaxed_matches, key=lambda x: x.text_similarity, reverse=True
            )[:3]
            best_match = min(
                top_matches,
                key=lambda x: (
                    x.matched_product.price
                    if x.matched_product.price > 0
                    else float("inf")
                ),
            )

            # ì´ë¯¸ì§€ ìœ ë¬´ì— ë”°ë¥¸ ë¡œê¹…
            if best_match.image_similarity > 0:
                self.logger.warning(
                    f"âš ï¸ ë‚®ì€ ì„ê³„ê°’ìœ¼ë¡œ ì´ë¯¸ì§€ ë§¤ì¹­: {best_match.matched_product.name} (í…ìŠ¤íŠ¸ ìœ ì‚¬ë„: {best_match.text_similarity:.2f}, ì´ë¯¸ì§€ ìœ ì‚¬ë„: {best_match.image_similarity:.2f}, ê°€ê²©: {best_match.matched_product.price})"
                )
            else:
                self.logger.warning(
                    f"âš ï¸ ë‚®ì€ ì„ê³„ê°’ìœ¼ë¡œ í…ìŠ¤íŠ¸ë§Œ ë§¤ì¹­: {best_match.matched_product.name} (í…ìŠ¤íŠ¸ ìœ ì‚¬ë„: {best_match.text_similarity:.2f}, ê°€ê²©: {best_match.matched_product.price})"
                )
            return best_match

        # ëª¨ë“  ì„ê³„ê°’ì—ì„œ ë§¤ì¹­ì„ ì°¾ì§€ ëª»í•œ ê²½ìš°, ëª¨ë“  ë§¤ì¹­ ì¤‘ ê°€ì¥ ìœ ì‚¬í•œ í•˜ë‚˜ ë°˜í™˜ (ë§¤ìš° ìœ ì—°í•œ ëŒ€ì•ˆ)
        if matches:
            # í†µí•© ìœ ì‚¬ë„ë¡œ ì •ë ¬í•´ ê°€ì¥ ë†’ì€ í•˜ë‚˜ ì„ íƒ
            best_match = max(matches, key=lambda x: x.combined_similarity)
            self.logger.warning(
                f"â— ëª¨ë“  ì„ê³„ê°’ ì‹¤íŒ¨, ê°€ì¥ ìœ ì‚¬í•œ ì œí’ˆ ì„ íƒ: {best_match.matched_product.name} (í†µí•© ìœ ì‚¬ë„: {best_match.combined_similarity:.2f})"
            )
            return best_match

        return None

    def process_files(
        self, input_files: List[str], output_dir: str = None, limit: int = None
    ) -> List[str]:
        """ì—¬ëŸ¬ íŒŒì¼ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤."""
        try:
            if not input_files:
                self.logger.error("ì…ë ¥ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
                return []

            # ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì •
            if output_dir:
                os.makedirs(output_dir, exist_ok=True)

            output_files = []
            for input_file in input_files:
                try:
                    # ì œí•œëœ ìˆ˜ì˜ ìƒí’ˆë§Œ ì²˜ë¦¬
                    if limit:
                        output_file = self._process_limited_file(
                            input_file, output_dir, limit
                        )
                    else:
                        output_file = self._process_single_file(input_file, output_dir)

                    if output_file:
                        output_files.append(output_file)

                except Exception as e:
                    self.logger.error(
                        f"íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {input_file}, ì˜¤ë¥˜: {str(e)}",
                        exc_info=True,
                    )
                    continue

            return output_files

        except Exception as e:
            self.logger.error(f"íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}", exc_info=True)
            return []

    def _process_limited_file(
        self, input_file: str, output_dir: str = None, limit: int = 10
    ) -> Optional[str]:
        """ì œí•œëœ ìˆ˜ì˜ ìƒí’ˆë§Œ ì²˜ë¦¬í•©ë‹ˆë‹¤."""
        # Reset running flag at the start of processing a file
        self._is_running = True
        try:
            start_time = datetime.now()
            self.logger.info(f"íŒŒì¼ ì²˜ë¦¬ ì‹œì‘: {input_file} (ìµœëŒ€ {limit}ê°œ ìƒí’ˆ)")

            # Excel íŒŒì¼ ì½ê¸°
            df = pd.read_excel(input_file)
            if df.empty:
                self.logger.error(f"íŒŒì¼ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤: {input_file}")
                return None

            # ë°ì´í„° ì •ì œ
            # Ensure data cleaning uses the configured cleaner
            df = self.data_cleaner.clean_dataframe(df)

            # ì œí•œëœ ìˆ˜ì˜ ìƒí’ˆë§Œ ì„ íƒ
            if len(df) > limit:
                self.logger.info(f"ì „ì²´ {len(df)}ê°œ ì¤‘ {limit}ê°œ ìƒí’ˆë§Œ ì²˜ë¦¬í•©ë‹ˆë‹¤.")
                df = df.head(limit)

            # ê²°ê³¼ ì €ì¥ì„ ìœ„í•œ ë¦¬ìŠ¤íŠ¸
            results = []
            total_items = len(df) # Get total items *after* limiting

            # Emit initial progress
            if self.progress_callback:
                 try:
                    self.progress_callback(0, total_items)
                 except Exception as cb_e:
                    self.logger.error(f"Error in initial progress callback: {cb_e}")

            # ê° ìƒí’ˆ ì²˜ë¦¬ (using ThreadPoolExecutor for potential future parallelization within limit)
            # Create futures list
            futures = []
            product_map = {} # To map future back to product if needed

            for idx, row in df.iterrows():
                 # Check if thread is still running before processing
                if not self._is_running:
                     self.logger.warning("Processing stopped by user request (limited file).")
                     break # Exit loop if stopped

                try:
                    # Create Product object
                    product = self.product_factory.create_product_from_row(row)
                    if not product:
                        self.logger.warning(f"Could not create product from row {idx+1}. Skipping.")
                        # Update progress even for skipped items
                        if self.progress_callback:
                            try:
                                self.progress_callback(idx + 1, total_items)
                            except Exception as cb_e:
                                self.logger.error(f"Error in progress callback (skipped item): {cb_e}")
                        continue

                    # Submit processing task
                    future = self.executor.submit(self._process_single_product, product)
                    futures.append(future)
                    product_map[future] = product # Store product associated with future

                except Exception as e:
                    self.logger.error(
                        f"ìƒí’ˆ ìƒì„±/ì œì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ (í–‰ {idx+1}): {str(e)}", exc_info=True
                    )
                    # Emit progress even on error to keep UI updated
                    if self.progress_callback:
                         try:
                             self.progress_callback(idx + 1, total_items)
                         except TypeError:
                             pass # Ignore signature mismatch error here too
                    continue

            # Process completed futures
            processed_count = 0
            from concurrent.futures import as_completed

            for future in as_completed(futures):
                 # Check if stopped before processing result
                 if not self._is_running:
                     self.logger.warning("Processing stopped during result collection (limited file).")
                     if not future.done():
                         future.cancel()
                     continue # Skip remaining futures

                 product = product_map.get(future) # Get the original product
                 processed_count += 1 # Increment counter for each future completed/checked

                 try:
                    result = future.result(timeout=300) # Get result
                    if result:
                        results.append(result)

                 except Exception as e:
                    error_msg = f"ìƒí’ˆ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ (Product ID: {product.id if product else 'N/A'}): {str(e)}"
                    self.logger.error(error_msg, exc_info=True)
                    # Optionally add an error result if needed
                    if product:
                         results.append(ProcessingResult(source_product=product, error=str(e)))

                 finally:
                    # --- Call progress callback ---
                    if self.progress_callback:
                        # Use try-except block to avoid crashing if callback signature mismatches
                        try:
                             # Use processed_count which reflects completed futures
                             self.progress_callback(processed_count, total_items)
                        except Exception as cb_e:
                             self.logger.error(f"Error in progress callback (limited file): {cb_e}")

            # If processing was stopped, skip output generation
            if not self._is_running:
                self.logger.warning("Processing was stopped. Skipping output file generation (limited file).")
                return None

            # ê²°ê³¼ê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ ì¶œë ¥ íŒŒì¼ ìƒì„±
            if results:
                # ì¶œë ¥ íŒŒì¼ëª… ìƒì„±
                output_file = self.excel_manager.generate_enhanced_output(
                    results, input_file, output_dir # Pass output_dir
                )

                # í›„ì²˜ë¦¬ ì‘ì—… ìˆ˜í–‰ (í•˜ì´í¼ë§í¬, í•„í„°ë§ ë“±)
                output_file = self.post_process_output_file(output_file)

                # ì²˜ë¦¬ ì™„ë£Œ ë¡œê¹…
                end_time = datetime.now()
                processing_time = end_time - start_time
                self.logger.info(
                    f"Limited processing finished at: {end_time.strftime('%Y-%m-%d %H:%M:%S')}"
                )
                self.logger.info(f"Total processing time (limited): {processing_time}")

                self.logger.info(f"íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ: {output_file}")
                return output_file

            else:
                self.logger.info("No results generated from limited processing.")
                return None

        except Exception as e:
            self.logger.error(f"íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}", exc_info=True)
            return None

    def process_koryo_products(self, search_query: str, output_path: str, max_items: int = 50):
        """Process Koryo Gift products and save to Excel"""
        try:
            self.logger.info(f"Starting Koryo Gift product processing for query: {search_query}")
            
            # ì œí’ˆ ê²€ìƒ‰ ë° ë°ì´í„° ìˆ˜ì§‘
            products = self.koryo_scraper.search_product(search_query, max_items=max_items)
            
            if not products:
                self.logger.warning(f"No products found for query: {search_query}")
                # ë¹ˆ ê²°ê³¼ë¥¼ ì €ì¥í•  ë•Œë„ í—¤ë”ì™€ ìƒíƒœ ë©”ì‹œì§€ í¬í•¨
                self.excel_manager.save_products([], output_path, "ê²€ìƒ‰ê²°ê³¼ì—†ìŒ")
                return
            
            self.logger.info(f"Found {len(products)} products from Koryo Gift")
            
            # ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì—‘ì…€ë¡œ ì €ì¥
            sheet_name = f"koryo_{datetime.now().strftime('%Y%m%d_%H%M')}"
            self.excel_manager.save_products(products, output_path, sheet_name)
            
            self.logger.info(f"Successfully saved Koryo Gift products to: {output_path}")
            
        except Exception as e:
            self.logger.error(f"Error processing Koryo Gift products: {str(e)}", exc_info=True)
            raise

    def process_search_results(self, search_query: str, output_path: str, max_items: int = 50):
        """Process search results from multiple sources"""
        try:
            all_products = []
            
            # ê³ ë ¤ê¸°í”„íŠ¸ ì œí’ˆ ê²€ìƒ‰
            try:
                koryo_products = self.koryo_scraper.search_product(search_query, max_items=max_items)
                if koryo_products:
                    self.logger.info(f"Found {len(koryo_products)} products from Koryo Gift")
                    all_products.extend(koryo_products)
                else:
                    self.logger.warning("No products found from Koryo Gift")
            except Exception as e:
                self.logger.error(f"Error searching Koryo Gift: {str(e)}")
            
            # ë„¤ì´ë²„ ì œí’ˆ ê²€ìƒ‰
            try:
                naver_products = self.naver_crawler.search_product(search_query, max_items=max_items)
                if naver_products:
                    self.logger.info(f"Found {len(naver_products)} products from Naver")
                    all_products.extend(naver_products)
                    
                    # ë„¤ì´ë²„ ê²€ìƒ‰ ê²°ê³¼ë§Œ ë³„ë„ë¡œ ì €ì¥
                    naver_sheet_name = f"naver_{datetime.now().strftime('%Y%m%d_%H%M')}"
                    naver_output_path = output_path.replace('.xlsx', '_naver.xlsx')
                    self.excel_manager.save_products(naver_products, naver_output_path, naver_sheet_name)
                    self.logger.info(f"Successfully saved Naver products to: {naver_output_path}")
                else:
                    self.logger.warning("No products found from Naver")
            except Exception as e:
                self.logger.error(f"Error searching Naver: {str(e)}")
            
            # ë‹¤ë¥¸ ì†ŒìŠ¤ì˜ ì œí’ˆ ê²€ìƒ‰ ë¡œì§...
            
            if not all_products:
                self.logger.warning(f"No products found for query: {search_query}")
                # ë¹ˆ ê²°ê³¼ë¥¼ ì €ì¥í•  ë•Œë„ í—¤ë”ì™€ ìƒíƒœ ë©”ì‹œì§€ í¬í•¨
                self.excel_manager.save_products([], output_path, "ê²€ìƒ‰ê²°ê³¼ì—†ìŒ")
                return
            
            # ëª¨ë“  ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì—‘ì…€ë¡œ ì €ì¥
            sheet_name = f"search_{datetime.now().strftime('%Y%m%d_%H%M')}"
            self.excel_manager.save_products(all_products, output_path, sheet_name)
            
            self.logger.info(f"Successfully saved all search results to: {output_path}")
            
        except Exception as e:
            self.logger.error(f"Error processing search results: {str(e)}", exc_info=True)
            raise

    def validate_product_data(self, product: Product) -> bool:
        """Validate product data before saving"""
        if not product:
            return False
            
        # í•„ìˆ˜ í•„ë“œ ê²€ì¦
        required_fields = {
            'name': product.name,
            'price': product.price,
            'url': product.url,
            'source': product.source
        }
        
        for field, value in required_fields.items():
            if not value:
                self.logger.warning(f"Missing required field: {field}")
                return False
        
        # ì´ë¯¸ì§€ URL ê²€ì¦
        if not product.image_url and not product.image_gallery:
            self.logger.warning("No images found for product")
            return False
            
        return True
